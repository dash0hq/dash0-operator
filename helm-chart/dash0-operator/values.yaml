# This file contains the default values for the Dash0 Kubernetes Operator Helm chart.
# Values can be overriden via --set or by providing additional yaml files when doing helm install etc.

# settings for the operator/controller
operator:
  # number of replica for the controller manager deployment
  replicaCount: 1

  # settings for the service account to be used
  serviceAccount:
    # whether to create a dedicated service account, set this to false if you want to provide a separately
    # created service account
    create: true
    # can be used to override the default name of the serviceaccount (defaults to "dash0-operator-controller-manager")
    name: ""

  # common labels, will be added to all operator resources, example:
  # additionalLabels:
  #   label1: "value 1"
  #   label2: "value 2"
  additionalLabels: {}

  # additional annotations for the controller manager service, example
  # serviceAnnotations:
  #   annotation1: "value 1"
  #   annotation2: "value 2"
  serviceAnnotations: {}

  # additional annotations for the controller manager deployment, example:
  # deploymentAnnotations:
  #   annotation1: "value 1"
  #   annotation2: "value 2"
  deploymentAnnotations: {}

  # additional annotations for the controller manager pod(s), example:
  # podAnnotations:
  #   annotation1: "value 1"
  #   annotation2: "value 2"
  podAnnotations: {}

  # additional labels for the controller manager pod(s), example:
  # podLabels:
  #   label1: "value 1"
  #   label2: "value 2"
  podLabels: {}

  # resources for the controller manager pod(s)
  managerPodResources:
    limits:
      cpu: 500m
      memory: 128Mi
      ephemeral-storage: 500Mi
    requests:
      cpu: 10m
      memory: 64Mi
      ephemeral-storage: 500Mi

  # resources for the kube-rbac-proxy pod(s)
  kubeRbacProxyPodResources:
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 5m
      memory: 64Mi

  # the port for the metrics service
  metricsPort: 8443

  # the port for the admission webhook service which instruments new workloads at deploy time
  webhookPort: 443

  # the container image to use for the controller manager component (there should usually be no reason to override this)
  image:
    # Use a different image entirely. Note that Dash0 does not offer support for Dash0 operator setups that do not use
    # Dash0's official image.
    repository: "dash0-operator-controller"
    # overrides the image tag, which defaults to the chart appVersion.
    tag:
    # override the default image pull policy
    pullPolicy:

  # the container image to use for the instrumentation init container
  # (there should usually be no reason to override this)
  initContainerImage:
    # Use a different image for the init container entirely. Note that Dash0 does not offer support for Dash0 operator
    # setups that do not use Dash0's official init container image.
    repository: "dash0-instrumentation"
    # overrides the image tag for the init container image
    tag: "1.0.0"
    # override the default image pull policy
    pullPolicy:

  # the image pull secrets to pull the container images
  imagePullSecrets: [ ]

  developmentMode: false

# settings for the OpenTelemetry collector instance that Dash0 will use for reporting data to Dash0
opentelemetry-collector:
  enabled: true

  image:
    repository: otel/opentelemetry-collector-k8s

  mode: daemonset

  service:
    enabled: true

  presets:
    kubernetesAttributes:
      enabled: true
    # kubeletMetrics:
    #   enabled: true

  additionalLabels:
    dash0.com/enable: "false"

  resources:
    limits:
      memory: 500Mi

  config:
    extensions:
      bearertokenauth/dash0:
        scheme: "Bearer"
        # needs to be provided via --set or an additional values file for now
        token:

    receivers:
      # Removing these receivers by setting them to null will print three warnings in helm 3.13.3 (and probaly other
      # 3.13.x versions) during helm install. They can be ignored safely. Probably related to
      # https://github.com/helm/helm/issues/12441 - not sure why that one is closed, apparently it is not fixed.
      # coalesce.go:286: warning: cannot overwrite table with non table for dash0-operator.opentelemetry-collector.config.receivers.jaeger (map[protocols:map[grpc:map[endpoint:${env:MY_POD_IP}:14250] thrift_compact:map[endpoint:${env:MY_POD_IP}:6831] thrift_http:map[endpoint:${env:MY_POD_IP}:14268]]])
      # coalesce.go:286: warning: cannot overwrite table with non table for dash0-operator.opentelemetry-collector.config.receivers.prometheus (map[config:map[scrape_configs:[map[job_name:opentelemetry-collector scrape_interval:10s static_configs:[map[targets:[${env:MY_POD_IP}:8888]]]]]]])
      # coalesce.go:286: warning: cannot overwrite table with non table for dash0-operator.opentelemetry-collector.config.receivers.zipkin (map[endpoint:${env:MY_POD_IP}:9411])
      jaeger: null
      prometheus: null
      zipkin: null

    exporters:
      otlp:
        auth:
          authenticator: bearertokenauth/dash0
        # needs to be provided via --set or an additional values file for now
        endpoint:

    service:
      pipelines:
        traces:
          receivers:
            - otlp
          processors:
            - k8sattributes
            - memory_limiter
            - batch
          exporters:
            - otlp
        metrics:
          receivers:
            - otlp
          processors:
            - k8sattributes
            - memory_limiter
            - batch
          exporters:
            - otlp
        logs:
          receivers:
            - otlp
          processors:
            - k8sattributes
            - memory_limiter
            - batch
          exporters:
            - otlp

  ports:
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
