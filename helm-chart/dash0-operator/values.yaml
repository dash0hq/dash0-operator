# This file contains the default values for the Dash0 Operator Helm chart.
# Values can be overriden via --set or by providing additional yaml files when running helm install.

# Settings for the Dash0 operator:
operator:

  # Use the operator.dash0Export settings to configure the connection to the Dash0 backend; telemetry will be sent to
  # the configured Dash0 backend by default. Under the hood, this will make sure a Dash0OperatorConfiguration resource
  # will be created right away, when starting the operator. If left empty, you can always create a
  # Dash0OperatorConfiguration resource manually later.
  dash0Export:
    # Set this to true to enable the creation of a Dash0OperatorConfiguration resource at startup. If a
    # Dash0OperatorConfiguration already exists in the cluster, no action will be taken. Note that if this is set to
    # true, you will also need to provide a valid endpoint (operator.dash0Export.endpoint), and either an auth token
    # (operator.dash0Export.token) or a reference to a Kubernetes secret containing that token
    # (operator.dash0Export.secretRef).
    enabled: false

    # The URL of the Dash0 ingress endpoint to which telemetry data will be sent. This property is mandatory if
    # operator.dash0Export.enabled is true, otherwise it will be ignored. The value needs to be the OTLP/gRPC endpoint
    # of your Dash0 organization. The correct OTLP/gRPC endpoint can be copied fom https://app.dash0.com -> organization
    # settings -> "Endpoints" -> "OTLP/gRPC". The correct endpoint value will always start with `ingress.` and end in
    # `dash0.com:4317`.
    endpoint:

    # The dataset to be used for reported telemetry and API access. This property is optional, it defaults to "default".
    # See https://www.dash0.com/documentation/dash0/key-concepts/datasets.
    dataset:

    # The base URL of the Dash0 API to talk to. This is not where telemetry will be sent, but it is used for managing
    # dashboards, check rules, synthetic checks and views via the operator. This property is mandatory. The value needs
    # to be the API endpoint of your Dash0 organization. The correct API endpoint can be copied fom
    # https://app.dash0.com -> organization settings -> "Endpoints" -> "API". The correct endpoint value will always
    # start with "https://api." and end in ".dash0.com"
    apiEndpoint:

    # The Dash0 authorization token. This property is optional, but either this property or the secretRef configuration
    # has to be provided if operator.dash0Export.enabled is true. If operator.dash0Export.enabled is false, this
    # property will be ignored.
    # If both token and secretRef are provided, the token will be used and secretRef will be ignored. The authorization
    # token for your Dash0 organization can be copied from https://app.dash0.com -> organization settings ->
    # "Auth Tokens".
    token:

    # A reference to a Kubernetes secret containing the Dash0 authorization token. This property is optional, and is
    # ignored if either operator.dash0Export.enabled is false or operator.dash0Export.token is set. The authorization
    # token for your Dash0 organization can be copied from https://app.dash0.com -> organization settings ->
    # "Auth Tokens".
    secretRef:
      # The name of the secret containing the Dash0 authorization token. Example: Assume you have created the Kubernetes
      # secret with the following command:
      # kubectl create secret generic \
      #   dash0-authorization-secret \
      #   --namespace dash0-system \
      #   --from-literal=token=auth_...your-token-here...
      #
      # Then you would set the property to "dash0-authorization-secret".
      name:
      # The key of the value which contains the Dash0 authorization token. Assuming you have created the Kubernetes
      # secret with the command above (see property "name"), then you would set the property to "token".
      key:

    # If secretRef is used, the Helm chart will check if the provided secret actually exists. Set this to true to
    # disable this check. This can be useful if you want to render the manifests via the Helm charts without accessing a
    # live Kubernetes cluster.
    disableSecretValidation: false

  # An opt-out for operator self-monitoring. If set to false, self monitoring for the operator will be disabled, that
  # is, the operator will not send self-monitoring telemetry to the configured Dash0 backend. This setting is optional,
  # it defaults to true.
  #
  # This setting has no effect if operator.dash0Export.enabled is false, as no Dash0OperatorConfiguration
  # resource will be created by the Helm chart then.
  selfMonitoringEnabled: true

  # An opt-out for collecting kubernetes infrastructure metrics. If set to false, the operator will not collect
  # Kubernetes infrastructure metrics. This setting is optional, it defaults to true.
  #
  # This setting has no effect if operator.dash0Export.enabled is false, as no Dash0OperatorConfiguration
  # resource will be created by the Helm chart then.
  kubernetesInfrastructureMetricsCollectionEnabled: true

  # An opt-out for collecting pod labels and annotations as resource attributes. If set to false, the operator will
  # not collect convert pod labels and annotations to resource attributes. This setting is optional, it defaults to
  # true.
  #
  # This setting has no effect if operator.dash0Export.enabled is false, as no Dash0OperatorConfiguration
  # resource will be created by the Helm chart then.
  collectPodLabelsAndAnnotationsEnabled: true

  # An opt-in for enabling support for Prometheus CRDs (PodMonitor, ServiceMonitor, ScrapeConfig). If set to true,
  # the operator will deploy an OpenTelemetry target-allocator that watches Prometheus CRDs and provides an API that
  # allows collectors to query the resulting scrape configs.
  #
  # This setting has no effect if operator.dash0Export.enabled is false, as no Dash0OperatorConfiguration
  # resource will be created by the Helm chart then.
  prometheusCrdSupportEnabled: false

  # If set, the value will be added as the resource attribute k8s.cluster.name to all telemetry. This setting is
  # optional. Per default, the resource attribute k8s.cluster.name will not be added.
  #
  # This setting has no effect if operator.dash0Export.enabled is false, as no Dash0OperatorConfiguration
  # resource will be created by the Helm chart then.
  clusterName: ""

  # The number of replicas for the operator manager deployment.
  replicaCount: 1

  # Settings for GKE clusters:
  gke:
    # Settings for GKE Autopilot clusters:
    autopilot:
      # Set operator.gke.autopilot.enabled=true if you are running the Dash0 operator in a GKE Autopilot cluster.
      # This will:
      # - deploy an AllowlistSynchronizer into the cluster
      # - disable collecting all four utilization metrics for the kubeletstats receiver metrics, collecting these
      #   requires access to the /pod endpoint of the kubelet API which is not available in GKE autopilot:
      #     - k8s.pod.cpu_limit_utilization
      #     - k8s.pod.cpu_request_utilization
      #     - k8s.pod.memory_limit_utilization
      #     - k8s.pod.memory_request_utilization
      # - disable collecting the extra metadata labels container.id and k8s.volume.type for the kubeletstats receiver
      #   metrics, collecting these requires access to the /pod endpoint of the kubelet API which is not available in
      #   GKE autopilot
      # - add additional namespaces to the NotIn namespaceSelectors to all three mutating webhooks; GKE autopilot would
      #   add these namespaces anyway
      enabled: false

      # Let the Dash0 operator Helm chart automatically deploy the AllowlistSynchronizer resource to your cluster, if
      # operator.gke.autopilot.enabled is true. This setting has no effect if operator.gke.autopilot.enabled is false.
      # If set to false, the assumption is that you have deployed the Dash0 AllowlistSynchronizer resource yourself into
      # your GKE Autopilot cluster.
      # The default is true, there should usually be no reason to override this.
      # See
      # https://github.com/dash0hq/dash0-operator/blob/main/helm-chart/dash0-operator/README.md#managing-the-allowlistsynchronizer-manually
      # for more information.
      deployAllowlistSynchronizer: true

  # An array of tolerations for the operator manager deployment. This can be used to make sure that the operator manager
  # pod(s) can be scheduled on nodes where they would not be scheduled otherwise due to Kubernetes taints.
  # Example:
  # tolerations:
  #   - key: "key1"
  #     operator: "Equal"
  #     value: "value1"
  #     effect: "NoSchedule"
  #   - key: "key2"
  #     operator: "Exists"
  #     effect: "NoSchedule"
  tolerations: []

  # The nodeAffinity for the operator manager deployment. This can be used to constrain which nodes the manager
  # pods will be scheduled on.
  #
  # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: "dash0.com/enable"
              operator: "NotIn"
              values: ["false"]
            - key: "kubernetes.io/os"
              operator: "In"
              values: ["linux"]

  # Settings for using cert-manager instead of auto-generating TLS certificates.
  #
  # See https://github.com/dash0hq/dash0-operator/blob/main/helm-chart/dash0-operator/README.md#using-cert-manager
  # for details on how to use cert-manager with the Dash0 operator.
  certManager:

    # Setting this to true disables the usage of automatically generated certificates by the Helm chart. If this is set
    # to true, the assumption is that certificates are managed by cert-manager (see https://cert-manager.io/),
    # and the other settings below (certManager.secretName, certManager.certManagerAnnotations) become required
    # settings, additionally it is recommended to set webhookService.name.
    useCertManager: false

    # The name of the secret used by cert-manager for the webhook certificate.
    # The provided name must match the `secretName` in the `cert-manager.io/v1.Certificate` resource's spec.
    # Note: This secret is created by cert-manager, you do not need to create it manually.
    secretName: null

    # A map of additional annotations that are added to all Kubernetes resources that need the webhook certificate.
    # Usually this will be a single `cert-manager.io/inject-ca-from` annotation with the namespace and name of the
    # Certificate resource, but other annotations are possible, see https://cert-manager.io/docs/reference/annotations/.
    certManagerAnnotations: {}

  webhookService:
    # A name override for the webhook service, defaults to dash0-operator-webhook-service.
    # If operator.certManager.useCertManager is set to true, the name of the webhook service (either the default name
    # or the name provided here) must match the DNS names provided in the cert-manager's Certificate resource.
    #
    # For that reason it is recommended to set this value when using cert-manager, as it guarantees that the names
    # match, even if the Dash0 operator helm chart would change the default name of the webhook service in a
    # future release.
    name: ""

    # the port for the webhook service
    port: 443

  # settings for the service account to be used
  serviceAccount:
    # whether to create a dedicated service account, set this to false if you want to provide a separately
    # created service account
    create: true
    # can be used to override the default name of the serviceaccount (defaults to "dash0-operator-controller")
    name: ""

  # common labels, will be added to all operator resources, example:
  # additionalLabels:
  #   label1: "value 1"
  #   label2: "value 2"
  additionalLabels: {}

  # additional annotations for the operator manager's metrics service, example
  # serviceAnnotations:
  #   annotation1: "value 1"
  #   annotation2: "value 2"
  serviceAnnotations: {}

  # additional annotations for the operator manager deployment, example:
  # deploymentAnnotations:
  #   annotation1: "value 1"
  #   annotation2: "value 2"
  deploymentAnnotations: {}

  # additional labels for the operator manager pod(s), example:
  # podLabels:
  #   label1: "value 1"
  #   label2: "value 2"
  podLabels: {}

  # additional annotations for the operator manager pod(s), example:
  # podAnnotations:
  #   annotation1: "value 1"
  #   annotation2: "value 2"
  podAnnotations: {}

  # resources for the operator manager container
  managerContainerResources:
    limits:
      cpu: 500m
      memory: 256Mi
      ephemeral-storage: 500Mi
    requests:
      cpu: 50m
      memory: 128Mi
      ephemeral-storage: 500Mi

  # The priority class name for the operator manager pods. If set, the operator manager pods will be scheduled with
  # the specified priority class. See https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/.
  # managerPriorityClassName:

  # Resource settings for the dash0-instrumentation init container that will be added to each pod when automatic
  # workload instrumentation is enabled.
  initContainerResources: {}
    # By default, this container defines no resource limits, but if required, they can be set here.
    # Note that putting a low CPU limit on the init container might slow down pod startup for instrumented pods
    # considerably.
    # limits:
    #   cpu:
    #   memory:
    #   ephemeral-storage:
    #
    # By default, this container defines no resource requests; if required, they can be set here. However, be aware that
    # setting requests for init containers might have unintended consequences. In particular, contrary to what one might
    # think intuitively, requested resources are _not_ freed up after the init container has completed. See the
    # discussion in https://github.com/kubernetes/kubernetes/issues/124282. This can make setting resource requests
    # rather expensive and lead to CPU/memory being needlessly reserved by the scheduler for the already finished init
    # container, although it will actually never be used again.
    #
    # requests:
    #   cpu: 100m
    #   memory: 150Mi
    #   ephemeral-storage:

  # the port for the metrics service
  metricsPort: 8443

  # the container image to use for the operator manager component (there should usually be no reason to override this)
  image:
    # Use a different image entirely. Note that Dash0 does not offer support for Dash0 operator setups that do not use
    # Dash0's official image.
    repository: "ghcr.io/dash0hq/operator-controller"
    # overrides the image tag, which defaults to the chart appVersion
    tag:
    # pull image by digest instead of tag; if this is set, the tag value will be ignored
    digest:
    # override the default image pull policy
    pullPolicy:

  # the container image to use for the instrumentation init container
  # (there should usually be no reason to override this)
  initContainerImage:
    # Use a different image for the init container entirely. Note that Dash0 does not offer support for Dash0 operator
    # setups that do not use Dash0's official init container image.
    repository: "ghcr.io/dash0hq/instrumentation"
    # overrides the image tag, which defaults to the chart appVersion.
    tag:
    # pull image by digest instead of tag; if this is set, the tag value will be ignored
    digest:
    # override the default image pull policy
    pullPolicy:

  # the container image to use for the collector component (there should usually be no reason to override this)
  collectorImage:
    repository: "ghcr.io/dash0hq/collector"
    # overrides the image tag, which defaults to the chart appVersion.
    tag:
    # pull image by digest instead of tag; if this is set, the tag value will be ignored
    digest:
    # override the default image pull policy
    pullPolicy:

  # the container image to use for the target-allocator component (there should usually be no reason to override this)
  targetAllocatorImage:
    repository: "ghcr.io/open-telemetry/opentelemetry-operator/target-allocator"
    # overrides the image tag, which defaults to the chart appVersion.
    tag: "0.140.0"
    # pull image by digest instead of tag; if this is set, the tag value will be ignored
    digest:
    # override the default image pull policy
    pullPolicy:

  # the container image to use for the configuration reloader of the collector component
  # (there should usually be no reason to override this)
  configurationReloaderImage:
    repository: "ghcr.io/dash0hq/configuration-reloader"
    # overrides the image tag, which defaults to the chart appVersion.
    tag:
    # pull image by digest instead of tag; if this is set, the tag value will be ignored
    digest:
    # override the default image pull policy
    pullPolicy:

  # the container image to use for synchronizing the offset files for the filelog receiver to the filelog offset config
  # map; will not be used if a filelog offset storage volume is provided via
  # operator.collectors.filelogOffsetSyncStorageVolume
  # (there should usually be no reason to override this)
  filelogOffsetSyncImage:
    repository: "ghcr.io/dash0hq/filelog-offset-sync"
    # overrides the image tag, which defaults to the chart appVersion.
    tag:
    # pull image by digest instead of tag; if this is set, the tag value will be ignored
    digest:
    # override the default image pull policy
    pullPolicy:

  # the container image to use for the init container which sets up the required file system ownership in the log file
  # offset volume; will only be used if a host volume is provided for filelog offset storage via
  # operator.collectors.filelogOffsetSyncStorageVolume
  # (there should usually be no reason to override this)
  filelogOffsetVolumeOwnershipImage:
    repository: "ghcr.io/dash0hq/filelog-offset-volume-ownership"
    # overrides the image tag, which defaults to the chart appVersion.
    tag:
    # pull image by digest instead of tag; if this is set, the tag value will be ignored
    digest:
    # override the default image pull policy
    pullPolicy:

  # the image pull secrets to pull the container images
  imagePullSecrets: []

  # If set to true, it enables a set of config changes suitable for local development or troubleshooting:
  # - instructs the logger (Zap) of the operator manager to use a Zap development config (stacktraces on warnings, no
  #   sampling), otherwise a Zap production config will be used (stacktraces on errors, sampling)
  # - adds a debug exporter (with the default config) to the collectors managed by the operator
  # - adds insecure_skip_verify: true to the kubeletstats receiver config, to enable the receiver to work on Docker
  #   Desktop and similar setups, see helm-chart/dash0-operator/README.md ->
  #   "Notes on Running The Operator on Docker Desktop"
  developmentMode: false

  # If set to a value, the operator manager will enable pprof and make it available on the specificied port.
  # Note that this setting is independent of operator.collectors.enablePprofExtension, which enables pprof for the
  # collectors.
  # The default is null, do not override this unless explicitly instructed by Dash0 support. Remove the override
  # after the operator troubleshooting is finished.
  pprofPort: null

  # Settings related to the modification and instrumentation of workloads in the cluster.
  instrumentation:
    # An optional delay to stagger access to the Kubernetes API server to instrument existing workloads at operator
    # startup or when enabling instrumentation for a new namespace via the Dash0Monitoring resource. This delay will be
    # applied after each individual workload.
    delayAfterEachWorkloadMillis: 0

    # An optional delay to stagger access to the Kubernetes API server to instrument (or update the instrumentation of)
    # existing workloads at operator startup. This delay will be applied each time all workloads in a namespace have been
    # processed, before starting with the next namespace.
    delayAfterEachNamespaceMillis: 0

    # If set to true, it enables verbose output from the init container and the injector.
    debug: false

  # If set to true, the operator manager will not watch the resources of the OpenTelemetry collectors it manages. By
  # default, the operator watches all collector resources (config maps, deployment, daemonset etc.) and reconciles them
  # if they are changed or deleted externally. This is generally the expected behavior of an operator in relation to the
  # resources it manages. For troubleshooting purposes, this can be disabled temporarily, allowing quick one-off edits
  # of the collector config maps.
  # Note that even if the watches are disabled, other events will still trigger a reconciliation of the collector
  # resources (for example changing the operator configuration resource, changing, adding or deleting monitoring
  # resources, and updating the operator to a new version).
  # The default is false, do not override this unless explicitly instructed by Dash0 support. Remove the override after
  # the collector troubleshooting is finished.
  disableCollectorResourceWatches: false

  # Settings related to the collectors managed by the operator.
  collectors:
    # If set to true, adds a debug exporter with "verbosity: detailed" to the collectors managed by the operator.
    # This setting produces a large amount of logs and is not suitable for extended production use.
    debugVerbosityDetailed: false

    # If set to a value > 0, the send_batch_max_size parameter will be set for the batch processor of the collectors
    # managed by the operator. There is usually no need to configure this. The value must be greater than or equal to
    # 8192, which is the default value for send_batch_size.
    sendBatchMaxSize: 0

    # Disables the replicaset informer for the k8sattributes processor. This will lead to not collecting the resource
    # attribute k8s.deployment.uid, and might lead to wrong values k8s.deployment.name in some edge cases.
    # The default is false, do not override this unless explicitly instructed by Dash0 support.
    disableReplicasetInformer: false

    # If set to true, instrumented workloads will be instructed to use the service URL of the OpenTelemetry collector
    # DaemonSet, instead of sending telemetry via a node-local route to the node's IP address and the collector's host
    # port. This can be useful if you employ mechanisms that block host ports, like certain Istio configurations.
    forceUseServiceUrl: false

    # If set to true, the host ports of the OpenTelemetry collector pods managed by the operator will be disaabled.
    # Implies forceUseServiceUrl: true.
    disableHostPorts: false

    # If set to true, the pprof extension will be enabled for both collectors.
    # The default is false, do not override this unless explicitly instructed by Dash0 support. Remove the override
    # after the collector troubleshooting is finished.
    enablePprofExtension: false

    # A reference to a persistent volume to use for the filelog offset sync. Specify this is you have a cluster
    # with more than pods 100 pods active at the same time. By default, the filelog offset sync will use a config map
    # for its peristent storage, but ConfigMaps are limited to 1 MB in size. If you have around 100 pods running in
    # your cluster, the recommendation is to provide volume.
    #
    # Here is an example with a hostPath volume (see also https://kubernetes.io/docs/concepts/storage/volumes/#hostpath
    # for considerations around using hostPath volumes):
    # operator:
    #   collectors:
    #    filelogOffsetSyncStorageVolume:
    #      name: filelogreceiver-offsets
    #      hostPath:
    #        path: /data/dash0-operator/offset-storage
    #        type: DirectoryOrCreate
    #
    # The directory in the hostPath volume will automatically be created with the correct permissions so that the
    # OpenTelemetry collector container can write to it.
    #
    # Here is another example based on persistent volume claims. (This assumes that a PersistentVolumeClaim named
    # offset-storage-claim exists.) See also
    # https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims and
    # https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming.
    # operator:
    #   collectors:
    #     filelogOffsetSyncStorageVolume:
    #       name: filelogreceiver-offsets
    #       persistentVolumeClaim:
    #         claimName: offset-storage-claim
    #
    # filelogOffsetSyncStorageVolume: ...

    # Resource settings for the collector container of the DaemonSet collector pods managed by the operator.
    # There should usually be no reason to override this.
    daemonSetCollectorContainerResources:
      limits:
        # cpu: (no cpu limit by default)
        memory: 500Mi
        # ephemeral-storage: (no ephemeral-storage limit by default)
      # The GOMEMLIMIT environment variable value for the collector container, the recommended value is 80% of
      # daemonSetCollectorContainerResources.limits.memory.
      gomemlimit: 400MiB
      requests:
        # cpu: (no cpu request by default)
        memory: 500Mi
        # ephemeral-storage: (no ephemeral-storage request by default)

    # Resource settings for the configuration reloader container of the DaemonSet collector pods managed by the operator.
    # There should usually be no reason to override this.
    daemonSetConfigurationReloaderContainerResources:
      limits:
        # cpu: (no cpu limit by default)
        memory: 24Mi
        # ephemeral-storage: (no ephemeral-storage limit by default)
      # The GOMEMLIMIT environment variable value for the configuration reloader container, the recommended value is 80%
      # of daemonSetConfigurationReloaderContainerResources.limits.memory.
      gomemlimit: 18MiB
      requests:
        # cpu: (no cpu request by default)
        memory: 12Mi
        # ephemeral-storage: (no ephemeral-storage request by default)

    # Resource settings for the filelog offset sync container of the DaemonSet collector pods managed by the operator.
    # There should usually be no reason to override this.
    daemonSetFileLogOffsetSyncContainerResources:
      limits:
        # cpu: (no cpu limit by default)
        memory: 32Mi
        # ephemeral-storage: (no ephemeral-storage limit by default)
      # The GOMEMLIMIT environment variable value for the filelog offset sync container, the recommended value is 80%
      # of daemonSetFileLogOffsetSyncContainerResources.limits.memory.
      gomemlimit: 24MiB
      requests:
        # cpu: (no cpu request by default)
        memory: 32Mi
        # ephemeral-storage: (no ephemeral-storage request by default)

    # An array of tolerations for the collector DaemonSet pods managed by the operator. This can be used to make sure
    # that collector pods are scheduled on nodes where they would not be scheduled otherwise due to Kubernetes taints.
    # Example:
    # daemonSetTolerations:
    #   - key: "key1"
    #     operator: "Equal"
    #     value: "value1"
    #     effect: "NoSchedule"
    #   - key: "key2"
    #     operator: "Exists"
    #     effect: "NoSchedule"
    daemonSetTolerations: []

    # The nodeAffinity for the collector DaemonSet pods managed by the operator. This can be used to constrain which
    # nodes the collector pods will be scheduled on.
    #
    # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    daemonSetNodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "dash0.com/enable"
                operator: "NotIn"
                values: ["false"]
              - key: "kubernetes.io/os"
                operator: "In"
                values: ["linux"]

    # The priority class name for the DaemonSet collector pods. If set, these manager pods will be scheduled with
    # the specified priority class.
    # This is useful if you want to make sure that the collector pods are scheduled with a higher priority than other
    # pods, that is, to ensure a collector pod is schedule on each node.
    # See https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/.
    # daemonSetPriorityClassName:

    # Resource settings for the collector container of the Deployment collector pod (cluster-metrics-collector) managed
    # by the operator. There should usually be no reason to override this.
    deploymentCollectorContainerResources:
      limits:
        # cpu: (no cpu limit by default)
        memory: 500Mi
        # ephemeral-storage: (no ephemeral-storage limit by default)
      # The GOMEMLIMIT environment variable value for the collector container, the recommended value is 80% of
      # deploymentCollectorContainerResources.limits.memory.
      gomemlimit: 400MiB
      requests:
        # cpu: (no cpu request by default)
        memory: 500Mi
        # ephemeral-storage: (no ephemeral-storage request by default)

    # Resource settings for the configuration reloader container of the Deployment collector pod
    # (cluster-metrics-collector) managed by the operator. There should usually be no reason to override this.
    deploymentConfigurationReloaderContainerResources:
      limits:
        # cpu: (no cpu limit by default)
        memory: 24Mi
        # ephemeral-storage: (no ephemeral-storage limit by default)
      # The GOMEMLIMIT environment variable value for the configuration reloader container, the recommended value is 80%
      # of deploymentConfigurationReloaderContainerResources.limits.memory.
      gomemlimit: 18MiB
      requests:
        # cpu: (no cpu request by default)
        memory: 12Mi
        # ephemeral-storage: (no ephemeral-storage request by default)

    # The priority class name for the Deployment collector pod. If set, this pod will be scheduled with the specified
    # priority class. See https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/.
    # deploymentPriorityClassName:

    # An array of tolerations for the collector Deployment pod (cluster-metrics-collector) managed by the operator.
    # This can be used to make sure that collector pod can be scheduled on nodes where it would not be scheduled
    # otherwise due to Kubernetes taints.
    # Example:
    # deploymentTolerations:
    #   - key: "key1"
    #     operator: "Equal"
    #     value: "value1"
    #     effect: "NoSchedule"
    #   - key: "key2"
    #     operator: "Exists"
    #     effect: "NoSchedule"
    deploymentTolerations: []

    # The nodeAffinity for the collector Deployment pod (cluster-metrics-collector) managed by the operator. This can
    # be used to constrain which nodes the collector pod will be scheduled on.
    #
    # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    deploymentNodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "dash0.com/enable"
                operator: "NotIn"
                values: ["false"]
              - key: "kubernetes.io/os"
                operator: "In"
                values: ["linux"]

  # Settings related to the target-allocator managed by the operator.
  targetAllocator:
    mTls:
      # Whether to enable mTLS for the communication between the target-allocator and the collectors.
      # If your metrics endpoints require authorization, enabling mTLS is required, so the credentials can be provided
      # to the collectors securily.
      # Enabling mTLS without providing the required secrets will result in a validation error.
      enabled: false
      # Name of the secret containing the server certificate.
      serverCertSecretName: ""
      # Name of the secret containing the client (collector) certificate.
      clientCertSecretName: ""

    # Resource settings for the target-allocator container of the target-allocator deployment managed by the operator.
    containerResources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 200m
        memory: 128Mi

    # An array of tolerations for the target-allocator deployment pods managed by the operator.
    # This can be used to make sure that target-allocator pod can be scheduled on nodes where it would not be scheduled
    # otherwise due to Kubernetes taints.
    # Example:
    # deploymentTolerations:
    #   - key: "key1"
    #     operator: "Equal"
    #     value: "value1"
    #     effect: "NoSchedule"
    #   - key: "key2"
    #     operator: "Exists"
    #     effect: "NoSchedule"
    tolerations: []

    # The nodeAffinity for the target-allocator deployment managed by the operator. This can be used to constrain which
    # nodes the target-allocator pod will be scheduled on.
    #
    # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "dash0.com/enable"
                operator: "NotIn"
                values: ["false"]
              - key: "kubernetes.io/os"
                operator: "In"
                values: ["linux"]
