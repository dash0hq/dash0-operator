#!/usr/bin/env bash

# SPDX-FileCopyrightText: Copyright 2024 Dash0 Inc.
# SPDX-License-Identifier: Apache-2.0

###############################################################################
# Utility functions for manual test scenarios.
###############################################################################

operator_namespace=${OPERATOR_NAMESPACE:-operator-namespace}

load_env_file () {
  local env_file="test-resources/.env"
  if [[ -f $env_file ]]; then
    echo "Loading env from file [$env_file]"
    source "$env_file"
  fi
  if [[ -z "${ALLOWED_KUBECTXS:-}" ]]; then
    echo "error: ALLOWED_KUBECTXS is not set, please set it in $env_file or via other means"
    exit 1
  fi

  if [[ -z "${LOCAL_REGISTRY_VOLUME_PATH:-}" ]]; then
    echo "warning: LOCAL_REGISTRY_VOLUME_PATH is not set, please set it in $env_file or via other means."
    exit 1
  fi

  if [[ -z "${DASH0_AUTHORIZATION_TOKEN:-}" ]]; then
    echo "warning: DASH0_AUTHORIZATION_TOKEN is not set. Exporting telemetry to Dash0 might not work; set it in $env_file or via other means."
  fi
  if [[ -z "${DASH0_INGRESS_ENDPOINT:-}" ]]; then
    echo "warning: DASH0_INGRESS_ENDPOINT is not set. Exporting telemetry to Dash0 might not work; set it in $env_file or via other means."
  fi
  if [[ -z "${DASH0_API_ENDPOINT:-}" ]]; then
    echo "warning: DASH0_API_ENDPOINT is not set. Synchronizing K8s resources via the Dash0 API might not work; set it in $env_file or via other means."
  fi
  # Provide default values for missing environment variables, otherwise scripts will fail with "XYZ: unbound variable"
  # later on, since we run scripts generally with set -u. The variables might not be needed at all, for example when
  # reporting telemetry to otlp-sink instead of an actual backend.
  export DASH0_INGRESS_ENDPOINT="${DASH0_INGRESS_ENDPOINT:-localhost:4317}"
  export DASH0_API_ENDPOINT="${DASH0_API_ENDPOINT:-http://localhost:8001}"
  export DASH0_AUTHORIZATION_TOKEN="${DASH0_AUTHORIZATION_TOKEN:-auth_token_not_set}"
}

verify_kubectx() {
  local current_kubectx=$(kubectl config current-context)
  local allowed_contexts_raw="${ALLOWED_KUBECTXS:-}"
  IFS=',' read -r -a allowed_contexts <<< "$allowed_contexts_raw"
  local current_context_is_allowed="false"
  for allowed_context_name in "${allowed_contexts[@]}"; do
    local trimmed=$(trim "$allowed_context_name")
    if [[ "$current_kubectx" = "$trimmed" ]]; then
      current_context_is_allowed="true"
      break
    fi
  done
  if [[ "$current_context_is_allowed" != "true" ]]; then
    echo "error: The current kube context \"$current_kubectx\" is not allowed for running local tests according to the setting ALLOWED_KUBECTXS (current value: \"$allowed_contexts_raw\"). Please switch to an allowed context or set ALLOWED_KUBECTXS differently (for example, in test-resources/.env). Refusing to run test scenarios/cleanup in context where that is potentially dangerous."
    exit 1
  fi
}

setup_test_environment () {
  if ! docker info > /dev/null 2>&1; then
    echo "This script uses docker, but it looks like Docker is not running. Please start docker and try again."
    exit 1
  fi

  test-resources/bin/render-templates.sh
  determine_container_images
  determine_test_app_images

  install_cert_manager_if_necessary
  install_nginx_ingress
}

ensure_namespace_exists() {
  local target_namespace=${1:-test-namespace}

  if [[ "${target_namespace}" = default  ]]; then
    exit 0
  fi

  if ! kubectl get ns "$target_namespace" &> /dev/null; then
    kubectl create ns "$target_namespace"
  fi
}

install_cert_manager_if_necessary() {
  if [[ "${USE_CERT_MANAGER:-}" = "true" ]]; then
    if ! kubectl get ns cert-manager &> /dev/null; then
      echo cert-manager namespace not found, deploying cert-manager
      test-resources/cert-manager/deploy.sh
    fi
    operator_webhook_service_name=custom-dash0-operator-webhook-service-name
  fi
}

install_nginx_ingress() {
  if [[ "${DEPLOY_NGINX_INGRESS:-}" = "false" ]]; then
    return
  fi
  echo "checking if ingress-nginx is already running..."
  if ! kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=5s; then
    echo "deploying ingress-nginx..."
    echo
    kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/deploy-ingress-nginx.yaml
    echo
    echo "waiting for ingress-nginx..."
    kubectl wait \
      --namespace ingress-nginx \
      --for=condition=ready \
      pod \
      --selector=app.kubernetes.io/component=controller \
      --timeout=90s
  else
    echo ingress-nginx is already running
  fi
}

is_remote_image() {
  image_name="$1"
  if [[ $image_name = */* ]]; then
    printf "true"
  else
    printf "false"
  fi
}

determine_container_images() {
  # The defaults when using the Helm chart from local sources are:
  # - use the locally built image with tag latest
  # - use pull policy "Always"
  # The defaults when using a remote helm chart are:
  # - default to not setting an explicit image repository or tag
  # - default to not setting a pull policy, instead let Kubernetes use the default pull policy

  local image_repository_prefix=${IMAGE_REPOSITORY_PREFIX:-}
  local image_tag=${IMAGE_TAG:-latest}
  local pull_policy=${PULL_POLICY:-Always}

  controller_image_repository="${image_repository_prefix}operator-controller"
  controller_image_tag="$image_tag"
  controller_image_digest=""
  controller_image_pull_policy="$pull_policy"
  instrumentation_image_repository="${image_repository_prefix}instrumentation"
  instrumentation_image_tag="$image_tag"
  instrumentation_image_digest=""
  instrumentation_image_pull_policy="$pull_policy"
  collector_image_repository="${image_repository_prefix}collector"
  collector_image_tag="$image_tag"
  collector_image_digest=""
  collector_image_pull_policy="$pull_policy"
  configuration_reloader_image_repository="${image_repository_prefix}configuration-reloader"
  configuration_reloader_image_tag="$image_tag"
  configuration_reloader_image_digest=""
  configuration_reloader_image_pull_policy="$pull_policy"
  filelog_offset_sync_image_repository="${image_repository_prefix}filelog-offset-sync"
  filelog_offset_sync_image_tag="$image_tag"
  filelog_offset_sync_image_digest=""
  filelog_offset_sync_image_pull_policy="$pull_policy"
  filelog_offset_volume_ownership_image_repository="${image_repository_prefix}filelog-offset-volume-ownership"
  filelog_offset_volume_ownership_image_tag="$image_tag"
  filelog_offset_volume_ownership_image_digest=""
  filelog_offset_volume_ownership_image_pull_policy="$pull_policy"

  operator_helm_chart=${OPERATOR_HELM_CHART:-helm-chart/dash0-operator}

  is_local_helm_chart="true"
  if [[ "$operator_helm_chart" != "helm-chart/dash0-operator" ]]; then
    echo "Looks like a remote helm chart or an installed Helm chart is used."
    is_local_helm_chart="false"

    controller_image_repository=""
    controller_image_tag=""
    controller_image_pull_policy=""
    instrumentation_image_repository=""
    instrumentation_image_tag=""
    instrumentation_image_pull_policy=""
    collector_image_repository=""
    collector_image_tag=""
    collector_image_pull_policy=""
    configuration_reloader_image_repository=""
    configuration_reloader_image_tag=""
    configuration_reloader_image_pull_policy=""
    filelog_offset_sync_image_repository=""
    filelog_offset_sync_image_tag=""
    filelog_offset_sync_image_pull_policy=""
    filelog_offset_volume_ownership_image_repository=""
    filelog_offset_volume_ownership_image_tag=""
    filelog_offset_volume_ownership_image_pull_policy=""
  fi

  if [[ -n "${CONTROLLER_IMAGE_REPOSITORY:-}" ]]; then
    controller_image_repository="$CONTROLLER_IMAGE_REPOSITORY"
  fi
  if [[ -n "${CONTROLLER_IMAGE_TAG:-}" ]]; then
    controller_image_tag="$CONTROLLER_IMAGE_TAG"
  fi
  if [[ -n "${CONTROLLER_IMAGE_DIGEST:-}" ]]; then
    controller_image_digest="$CONTROLLER_IMAGE_DIGEST"
  fi
  if [[ -n "${CONTROLLER_IMAGE_PULL_POLICY:-}" ]]; then
    controller_image_pull_policy="$CONTROLLER_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${INSTRUMENTATION_IMAGE_REPOSITORY:-}" ]]; then
    instrumentation_image_repository="$INSTRUMENTATION_IMAGE_REPOSITORY"
  fi
  if [[ -n "${INSTRUMENTATION_IMAGE_TAG:-}" ]]; then
    instrumentation_image_tag="$INSTRUMENTATION_IMAGE_TAG"
  fi
  if [[ -n "${INSTRUMENTATION_IMAGE_DIGEST:-}" ]]; then
    instrumentation_image_digest="$INSTRUMENTATION_IMAGE_DIGEST"
  fi
  if [[ -n "${INSTRUMENTATION_IMAGE_PULL_POLICY:-}" ]]; then
    instrumentation_image_pull_policy="$INSTRUMENTATION_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${COLLECTOR_IMAGE_REPOSITORY:-}" ]]; then
    collector_image_repository="$COLLECTOR_IMAGE_REPOSITORY"
  fi
  if [[ -n "${COLLECTOR_IMAGE_TAG:-}" ]]; then
    collector_image_tag="$COLLECTOR_IMAGE_TAG"
  fi
  if [[ -n "${COLLECTOR_IMAGE_DIGEST:-}" ]]; then
    collector_image_digest="$COLLECTOR_IMAGE_DIGEST"
  fi
  if [[ -n "${COLLECTOR_IMAGE_PULL_POLICY:-}" ]]; then
    collector_image_pull_policy="$COLLECTOR_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${CONFIGURATION_RELOADER_IMAGE_REPOSITORY:-}" ]]; then
    configuration_reloader_image_repository="$CONFIGURATION_RELOADER_IMAGE_REPOSITORY"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMAGE_TAG:-}" ]]; then
    configuration_reloader_image_tag="$CONFIGURATION_RELOADER_IMAGE_TAG"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMAGE_DIGEST:-}" ]]; then
    configuration_reloader_image_digest="$CONFIGURATION_RELOADER_IMAGE_DIGEST"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMAGE_PULL_POLICY:-}" ]]; then
    configuration_reloader_image_pull_policy="$CONFIGURATION_RELOADER_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${FILELOG_OFFSET_SYNC_IMAGE_REPOSITORY:-}" ]]; then
    filelog_offset_sync_image_repository="$FILELOG_OFFSET_SYNC_IMAGE_REPOSITORY"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMAGE_TAG:-}" ]]; then
    filelog_offset_sync_image_tag="$FILELOG_OFFSET_SYNC_IMAGE_TAG"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMAGE_DIGEST:-}" ]]; then
    filelog_offset_sync_image_digest="$FILELOG_OFFSET_SYNC_IMAGE_DIGEST"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMAGE_PULL_POLICY:-}" ]]; then
    filelog_offset_sync_image_pull_policy="$FILELOG_OFFSET_SYNC_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_REPOSITORY:-}" ]]; then
    filelog_offset_volume_ownership_image_repository="$FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_REPOSITORY"
  fi
  if [[ -n "${FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_TAG:-}" ]]; then
    filelog_offset_volume_ownership_image_tag="$FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_TAG"
  fi
  if [[ -n "${FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_DIGEST:-}" ]]; then
    filelog_offset_volume_ownership_image_digest="$FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_DIGEST"
  fi
  if [[ -n "${FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_PULL_POLICY:-}" ]]; then
    filelog_offset_volume_ownership_image_pull_policy="$FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_PULL_POLICY"
  fi

  if [[ "$is_local_helm_chart" = "true" ]]; then
    # support using the local helm chart with remote images
    controller_image_pull_policy=$(determine_pull_policy "$controller_image_repository" "$controller_image_pull_policy")
    instrumentation_image_pull_policy=$(determine_pull_policy "$instrumentation_image_repository" "$instrumentation_image_pull_policy")
    collector_image_pull_policy=$(determine_pull_policy "$collector_image_repository" "$collector_image_pull_policy")
    configuration_reloader_image_pull_policy=$(determine_pull_policy "$configuration_reloader_image_repository" "$configuration_reloader_image_pull_policy")
    filelog_offset_sync_image_pull_policy=$(determine_pull_policy "$filelog_offset_sync_image_repository" "$filelog_offset_sync_image_pull_policy")
    filelog_offset_volume_ownership_image_pull_policy=$(determine_pull_policy "$filelog_offset_volume_ownership_image_repository" "$filelog_offset_volume_ownership_image_pull_policy")
  fi
}

determine_pull_policy() {
  local image_repo="$1"
  local pp="$2"
  if [[ $(is_remote_image "$image_repo") = "true" ]]; then
    echo ""
  else
    echo "$pp"
  fi
}

determine_test_app_images() {
  local image_repository_prefix=${TEST_IMAGE_REPOSITORY_PREFIX:-}
  local image_tag=${TEST_IMAGE_TAG:-latest}
  local pull_policy=${TEST_IMAGE_PULL_POLICY:-Always}

  test_app_dotnet_image_repository="${image_repository_prefix}dash0-operator-dotnet-test-app"
  test_app_dotnet_image_tag="$image_tag"
  test_app_dotnet_image_pull_policy="$pull_policy"
  test_app_jvm_image_repository="${image_repository_prefix}dash0-operator-jvm-spring-boot-test-app"
  test_app_jvm_image_tag="$image_tag"
  test_app_jvm_image_pull_policy="$pull_policy"
  test_app_nodejs_image_repository="${image_repository_prefix}dash0-operator-nodejs-20-express-test-app"
  test_app_nodejs_image_tag="$image_tag"
  test_app_nodejs_image_pull_policy="$pull_policy"
  test_app_python_image_repository="${image_repository_prefix}dash0-operator-python-flask-test-app"
  test_app_python_image_tag="$image_tag"
  test_app_python_image_pull_policy="$pull_policy"

  if [[ -n "${TEST_APP_DOTNET_IMAGE_REPOSITORY:-}" ]]; then
    test_app_dotnet_image_repository="$TEST_APP_DOTNET_IMAGE_REPOSITORY"
  fi
  if [[ -n "${TEST_APP_DOTNET_IMAGE_TAG:-}" ]]; then
    test_app_dotnet_image_tag="$TEST_APP_DOTNET_IMAGE_TAG"
  fi
  if [[ -n "${TEST_APP_DOTNET_IMAGE_PULL_POLICY:-}" ]]; then
    test_app_dotnet_image_pull_policy="$TEST_APP_DOTNET_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${TEST_APP_JVM_IMAGE_REPOSITORY:-}" ]]; then
    test_app_jvm_image_repository="$TEST_APP_JVM_IMAGE_REPOSITORY"
  fi
  if [[ -n "${TEST_APP_JVM_IMAGE_TAG:-}" ]]; then
    test_app_jvm_image_tag="$TEST_APP_JVM_IMAGE_TAG"
  fi
  if [[ -n "${TEST_APP_JVM_IMAGE_PULL_POLICY:-}" ]]; then
    test_app_jvm_image_pull_policy="$TEST_APP_JVM_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${TEST_APP_NODEJS_IMAGE_REPOSITORY:-}" ]]; then
    test_app_nodejs_image_repository="$TEST_APP_NODEJS_IMAGE_REPOSITORY"
  fi
  if [[ -n "${TEST_APP_NODEJS_IMAGE_TAG:-}" ]]; then
    test_app_nodejs_image_tag="$TEST_APP_NODEJS_IMAGE_TAG"
  fi
  if [[ -n "${TEST_APP_NODEJS_IMAGE_PULL_POLICY:-}" ]]; then
    test_app_nodejs_image_pull_policy="$TEST_APP_NODEJS_IMAGE_PULL_POLICY"
  fi

  if [[ -n "${TEST_APP_PYTHON_IMAGE_REPOSITORY:-}" ]]; then
    test_app_python_image_repository="$TEST_APP_PYTHON_IMAGE_REPOSITORY"
  fi
  if [[ -n "${TEST_APP_PYTHON_IMAGE_TAG:-}" ]]; then
    test_app_python_image_tag="$TEST_APP_PYTHON_IMAGE_TAG"
  fi
  if [[ -n "${TEST_APP_PYTHON_IMAGE_PULL_POLICY:-}" ]]; then
    test_app_python_image_pull_policy="$TEST_APP_PYTHON_IMAGE_PULL_POLICY"
  fi
}

build_all_images() {
  if [[ "${SKIP_IMAGE_BUILDS:-}" = "true" ]]; then
    return
  fi

  CONTROLLER_IMAGE_REPOSITORY="$controller_image_repository" \
    CONTROLLER_IMAGE_TAG="$controller_image_tag" \
    INSTRUMENTATION_IMAGE_REPOSITORY="$instrumentation_image_repository" \
    INSTRUMENTATION_IMAGE_TAG="$instrumentation_image_tag" \
    COLLECTOR_IMAGE_REPOSITORY="$collector_image_repository" \
    CONFIGURATION_RELOADER_IMAGE_REPOSITORY="$configuration_reloader_image_repository" \
    CONFIGURATION_RELOADER_IMAGE_TAG="$configuration_reloader_image_tag" \
    FILELOG_OFFSET_SYNC_IMAGE_REPOSITORY=$filelog_offset_sync_image_repository \
    FILELOG_OFFSET_SYNC_IMAGE_TAG="$filelog_offset_sync_image_tag" \
    FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_REPOSITORY=$filelog_offset_volume_ownership_image_repository \
    FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_TAG="$filelog_offset_volume_ownership_image_tag" \
    make images
}

push_all_images() {
  if [[ "${PUSH_BUILT_IMAGES:-}" != "true" ]]; then
    return
  fi

  CONTROLLER_IMAGE_REPOSITORY="$controller_image_repository" \
    CONTROLLER_IMAGE_TAG="$controller_image_tag" \
    INSTRUMENTATION_IMAGE_REPOSITORY="$instrumentation_image_repository" \
    INSTRUMENTATION_IMAGE_TAG="$instrumentation_image_tag" \
    COLLECTOR_IMAGE_REPOSITORY="$collector_image_repository" \
    CONFIGURATION_RELOADER_IMAGE_REPOSITORY="$configuration_reloader_image_repository" \
    CONFIGURATION_RELOADER_IMAGE_TAG="$configuration_reloader_image_tag" \
    FILELOG_OFFSET_SYNC_IMAGE_REPOSITORY=$filelog_offset_sync_image_repository \
    FILELOG_OFFSET_SYNC_IMAGE_TAG="$filelog_offset_sync_image_tag" \
    FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_REPOSITORY=$filelog_offset_volume_ownership_image_repository \
    FILELOG_OFFSET_VOLUME_OWNERSHIP_IMAGE_TAG="$filelog_offset_volume_ownership_image_tag" \
    make push-images
}

deploy_filelog_offsets_pvc() {
  echo "STEP $step_counter: install filelog offset persistent volume claim"
  if [[ "$is_kind_cluster" = "true" ]]; then
    kubectl patch persistentvolume offset-storage-volume --type=json -p='[{"op": "remove", "path": "/spec/claimRef"}]' || true
    kubectl apply -f test-resources/customresources/filelogoffsetpvc/offset-storage-pvc-kind.yaml
  else
    kubectl apply -f test-resources/customresources/filelogoffsetpvc/offset-storage-pvc-docker.yaml
  fi
}

deploy_additional_resources() {
  deploy_priority_classes
  deploy_additional_cert_manager_resources
}

deploy_priority_classes() {
  echo "STEP $step_counter: deploying priority classes"
  kubectl apply -f test-resources/customresources/priorityclass/priorityclasses.yaml
  finish_step
}

deploy_additional_cert_manager_resources() {
  if [[ "${USE_CERT_MANAGER:-}" = "true" ]]; then
    echo "STEP $step_counter: install certificate and issuer for cert-manager"
    kubectl apply -n "$operator_namespace" -f test-resources/cert-manager/certificate-and-issuer.yaml
    finish_step
  fi
}

deploy_via_helm() {
  run_helm install
}

update_via_helm() {
  run_helm upgrade
}

run_helm() {
  deploy_otlp_sink_if_requested

  local action=${1:-install}
  local helm_install_command="helm $action --wait --namespace $operator_namespace"
  if [[ -n "${OPERATOR_HELM_CHART_VERSION:-}" ]]; then
    helm_install_command+=" --version $OPERATOR_HELM_CHART_VERSION"
  fi
  if [[ -e test-resources/bin/extra-values.yaml ]]; then
    helm_install_command+=" --values test-resources/bin/extra-values.yaml"
  fi
  if [[ "${USE_CERT_MANAGER:-}" = "true" ]]; then
    helm_install_command+=" --values test-resources/cert-manager/helm-values.yaml"
  fi
  helm_install_command+=" --set operator.developmentMode=true"
  if [[ "${OTEL_COLLECTOR_DEBUG_VERBOSITY_DETAILED:-}" = "true" ]]; then
    helm_install_command+=" --set operator.collectors.debugVerbosityDetailed=true"
  fi
  if [[ -n "${OTEL_COLLECTOR_SEND_BATCH_MAX_SIZE:-}" ]]; then
    helm_install_command+=" --set operator.collectors.sendBatchMaxSize=$OTEL_COLLECTOR_SEND_BATCH_MAX_SIZE"
  fi
  if [[ "${FILELOG_OFFSETS_PVC:-}" = "true" ]]; then
    helm_install_command+=" --values test-resources/bin/filelog-offset-volume-pvc-values.yaml"
  elif [[ "${FILELOG_OFFSETS_HOST_PATH_VOLUME:-}" = "true" ]]; then
    helm_install_command+=" --values test-resources/bin/filelog-offset-volume-hostpath-values.yaml"
  fi

  if [[ -n "$controller_image_repository" ]]; then
    helm_install_command+=" --set operator.image.repository=$controller_image_repository"
  fi
  if [[ -n "$controller_image_tag" ]]; then
    helm_install_command+=" --set operator.image.tag=$controller_image_tag"
  fi
  if [[ -n "$controller_image_digest" ]]; then
    helm_install_command+=" --set operator.image.digest=$controller_image_digest"
  fi
  if [[ -n "$controller_image_pull_policy" ]]; then
    helm_install_command+=" --set operator.image.pullPolicy=$controller_image_pull_policy"
  fi

  if [[ -n "$instrumentation_image_repository" ]]; then
    helm_install_command+=" --set operator.initContainerImage.repository=$instrumentation_image_repository"
  fi
  if [[ -n "$instrumentation_image_tag" ]]; then
    helm_install_command+=" --set operator.initContainerImage.tag=$instrumentation_image_tag"
  fi
  if [[ -n "$instrumentation_image_digest" ]]; then
    helm_install_command+=" --set operator.initContainerImage.digest=$instrumentation_image_digest"
  fi
  if [[ -n "$instrumentation_image_pull_policy" ]]; then
    helm_install_command+=" --set operator.initContainerImage.pullPolicy=$instrumentation_image_pull_policy"
  fi

  if [[ -n "$collector_image_repository" ]]; then
    helm_install_command+=" --set operator.collectorImage.repository=$collector_image_repository"
  fi
  if [[ -n "$collector_image_tag" ]]; then
    helm_install_command+=" --set operator.collectorImage.tag=$collector_image_tag"
  fi
  if [[ -n "$collector_image_digest" ]]; then
    helm_install_command+=" --set operator.collectorImage.digest=$collector_image_digest"
  fi
  if [[ -n "$collector_image_pull_policy" ]]; then
    helm_install_command+=" --set operator.collectorImage.pullPolicy=$collector_image_pull_policy"
  fi

  if [[ -n "$configuration_reloader_image_repository" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.repository=$configuration_reloader_image_repository"
  fi
  if [[ -n "$configuration_reloader_image_tag" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.tag=$configuration_reloader_image_tag"
  fi
  if [[ -n "$configuration_reloader_image_digest" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.digest=$configuration_reloader_image_digest"
  fi
  if [[ -n "$configuration_reloader_image_pull_policy" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.pullPolicy=$configuration_reloader_image_pull_policy"
  fi

  if [[ -n "$filelog_offset_sync_image_repository" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.repository=$filelog_offset_sync_image_repository"
  fi
  if [[ -n "$filelog_offset_sync_image_tag" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.tag=$filelog_offset_sync_image_tag"
  fi
  if [[ -n "$filelog_offset_sync_image_digest" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.digest=$filelog_offset_sync_image_digest"
  fi
  if [[ -n "$filelog_offset_sync_image_pull_policy" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.pullPolicy=$filelog_offset_sync_image_pull_policy"
  fi

  if [[ -n "$filelog_offset_volume_ownership_image_repository" ]]; then
    helm_install_command+=" --set operator.filelogOffsetVolumeOwnershipImage.repository=$filelog_offset_volume_ownership_image_repository"
  fi
  if [[ -n "$filelog_offset_volume_ownership_image_tag" ]]; then
    helm_install_command+=" --set operator.filelogOffsetVolumeOwnershipImage.tag=$filelog_offset_volume_ownership_image_tag"
  fi
  if [[ -n "$filelog_offset_volume_ownership_image_digest" ]]; then
    helm_install_command+=" --set operator.filelogOffsetVolumeOwnershipImage.digest=$filelog_offset_volume_ownership_image_digest"
  fi
  if [[ -n "$filelog_offset_volume_ownership_image_pull_policy" ]]; then
    helm_install_command+=" --set operator.filelogOffsetVolumeOwnershipImage.pullPolicy=$filelog_offset_volume_ownership_image_pull_policy"
  fi

  # Deploy an operator configuration right away.
  if [[ "${DEPLOY_OPERATOR_CONFIGURATION_VIA_HELM:-}" != "false" ]]; then
    if [[ "${USE_OTLP_SINK:-}" = "true" ]]; then
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=http://otlp-sink.otlp-sink.svc.cluster.local:4317"
      helm_install_command+=" --set operator.dash0Export.token=dummy-token"
      # Note: API synchronization (Perses dashboards, Prometheus rules) are not supported when using the OTLP sink.
    else
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=${DASH0_INGRESS_ENDPOINT}"
      if [[ "${USE_TOKEN:-}" = true ]]; then
        helm_install_command+=" --set operator.dash0Export.token=${DASH0_AUTHORIZATION_TOKEN}"
      else
        helm_install_command+=" --set operator.dash0Export.secretRef.name=dash0-authorization-secret"
        helm_install_command+=" --set operator.dash0Export.secretRef.key=token"
      fi
      helm_install_command+=" --set operator.dash0Export.apiEndpoint=${DASH0_API_ENDPOINT}"
    fi
    if [[ -n "${OPERATOR_CONFIGURATION_VIA_HELM_DATASET:-}" ]]; then
      helm_install_command+=" --set operator.dash0Export.dataset=$OPERATOR_CONFIGURATION_VIA_HELM_DATASET"
    fi
    if [[ "${SELF_MONITORING_ENABLED:-}" = "false" ]]; then
      helm_install_command+=" --set operator.selfMonitoringEnabled=false"
    fi
    if [[ "${KUBERNETES_INFRASTRUCTURE_METRICS_COLLECTION_ENABLED:-}" = "false" ]]; then
      helm_install_command+=" --set operator.kubernetesInfrastructureMetricsCollectionEnabled=false"
    fi
    helm_install_command+=" --set operator.clusterName=$(kubectl config current-context)"
  fi
  if [[ -n "${OPERATOR_REPLICAS:-}" ]]; then
    helm_install_command+=" --set operator.replicaCount=${OPERATOR_REPLICAS:-1}"
  fi

  helm_install_command+=" dash0-operator"
  helm_install_command+=" ${OPERATOR_HELM_CHART:-helm-chart/dash0-operator}"
  echo Helm install command:
  echo "$helm_install_command"
  # helm install --wait can take a couple of seconds, since the readiness check and the Helm post-install hook make sure
  # that the operator deployment is actually ready, and a monitoring resource can be deployed immediately afterwards.
  # Executing the command with `time` to get some feedback about the duration.
  time $helm_install_command
}

deploy_otlp_sink_if_requested() {
  if [[ "${USE_OTLP_SINK:-}" != "true" ]]; then
    return
  fi

  echo "deploying otlp-sink"
  tmpfile=$(mktemp "${TMPDIR}otlp-sink-yaml.XXXX")
  trap 'rm -f "$tmpfile"' EXIT
  local otlp_sink_dir="$(realpath "$(pwd)")/test-resources/e2e/volumes/otlp-sink"
  sed "s|/tmp/telemetry|$otlp_sink_dir|g" test-resources/otlp-sink/otlp-sink.yaml > "$tmpfile"

  mkdir -p "$otlp_sink_dir"
  echo "deleting old OTLP sink old telemetry dump files"
  rm -rf "$otlp_sink_dir/traces.jsonl"
  rm -rf "$otlp_sink_dir/metrics.jsonl"
  rm -rf "$otlp_sink_dir/logs.jsonl"
  echo "creating OTLP sink old telemetry dump files"
  touch "$otlp_sink_dir/traces.jsonl"
  touch "$otlp_sink_dir/metrics.jsonl"
  touch "$otlp_sink_dir/logs.jsonl"

  kubectl apply -f "$tmpfile"
  kubectl rollout status \
    deployment \
    otlp-sink \
    --namespace otlp-sink \
    --timeout 1m
}

install_operator_configuration_resource() {
  if [[ "${USE_OTLP_SINK:-}" = "true" ]]; then
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.otlpsink.yaml
  elif [[ "${USE_TOKEN:-}" = true ]]; then
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.token.yaml
  else
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.secret.yaml
  fi

  echo "waiting for the operator configuration resource to become available"
  kubectl wait dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-resource --for condition=Available
}

install_monitoring_resource() {
  local additional_namespaces=${1:-false}
  kubectl apply -n ${target_namespace} -f test-resources/customresources/dash0monitoring/dash0monitoring.yaml
  if [[ "$additional_namespaces" = "true" ]]; then
    kubectl apply -n test-namespace-2 -f test-resources/customresources/dash0monitoring/dash0monitoring-2.yaml
    kubectl apply -n test-namespace-3 -f test-resources/customresources/dash0monitoring/dash0monitoring-3.yaml
  fi

  echo "waiting for the monitoring resource to become available"
  kubectl wait --namespace ${target_namespace} dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
  if [[ "$additional_namespaces" = "true" ]]; then
    kubectl wait --namespace test-namespace-2 dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
  fi
}

deploy_application_under_monitoring() {
  if [[ "${DEPLOY_APPLICATION_UNDER_MONITORING:-}" = "false" ]]; then
    return
  fi

  runtime_under_test="${1:-nodejs}"
  echo "STEP $step_counter: deploy application under monitoring ($runtime_under_test)"
  if [[ "${runtime_under_test:-}" = "nodejs" ]]; then
    build_test_application test-resources/node.js/express "$test_app_nodejs_image_repository"
    push_test_application_image "$test_app_nodejs_image_repository"
    helm_install_test_application \
      test-resources/node.js/express/dash0-operator-test-app-nodejs \
      test-app-nodejs \
      "$target_namespace" \
      "$kind" \
      "$test_app_nodejs_image_repository" \
      "$test_app_nodejs_image_tag" \
      "$test_app_nodejs_image_pull_policy"
    if [[ "$additional_namespaces" = true ]]; then
      helm_install_test_application \
        test-resources/node.js/express/dash0-operator-test-app-nodejs \
        test-app-nodejs-2 \
        test-namespace-2 \
        daemonset \
        "$test_app_nodejs_image_repository" \
        "$test_app_nodejs_image_tag" \
        "$test_app_nodejs_image_pull_policy"
      helm_install_test_application \
        test-resources/node.js/express/dash0-operator-test-app-nodejs \
        test-app-nodejs-3 \
        test-namespace-3 \
        statefulset \
        "$test_app_nodejs_image_repository" \
        "$test_app_nodejs_image_tag" \
        "$test_app_nodejs_image_pull_policy"
    fi
  elif [[ "${runtime_under_test:-}" = "jvm" ]]; then
    build_test_application test-resources/jvm/spring-boot "$test_app_jvm_image_repository"
    push_test_application_image "$test_app_jvm_image_repository"
    helm_install_test_application \
      "test-resources/jvm/spring-boot/dash0-operator-test-app-jvm" \
      "test-app-jvm" \
      "${target_namespace}" \
      "${kind}"
      "$test_app_jvm_image_repository" \
      "$test_app_jvm_image_tag" \
      "$test_app_jvm_image_pull_policy"
    if [[ "$additional_namespaces" = true ]]; then
      helm_install_test_application \
        "test-resources/jvm/spring-boot/dash0-operator-test-app-jvm" \
        "test-app-jvm-ns2" \
        "test-namespace-2" \
        "daemonset"
        "$test_app_jvm_image_repository" \
        "$test_app_jvm_image_tag" \
        "$test_app_jvm_image_pull_policy"
      helm_install_test_application \
        "test-resources/jvm/spring-boot/dash0-operator-test-app-jvm" \
        "test-app-jvm-ns3" \
        "test-namespace-3" \
        "statefulset" \
        "$test_app_jvm_image_repository" \
        "$test_app_jvm_image_tag" \
        "$test_app_jvm_image_pull_policy"
    fi
  elif [[ "${runtime_under_test:-}" = "dotnet" ]]; then
    build_test_application test-resources/dotnet "$test_app_dotnet_image_repository"
    push_test_application_image "$test_app_dotnet_image_repository"
    helm_install_test_application \
      "test-resources/dotnet/dash0-operator-test-app-dotnet" \
      "test-app-dotnet" \
      "${target_namespace}" \
      "${kind}"
      "$test_app_dotnet_image_repository" \
      "$test_app_dotnet_image_tag" \
      "$test_app_dotnet_image_pull_policy"
    if [[ "$additional_namespaces" = true ]]; then
      helm_install_test_application \
        "test-resources/dotnet/dash0-operator-test-app-dotnet" \
        "test-app-dotnet-ns2" \
        "test-namespace-2" \
        "daemonset" \
        "$test_app_dotnet_image_repository" \
        "$test_app_dotnet_image_tag" \
        "$test_app_dotnet_image_pull_policy"
      helm_install_test_application \
        "test-resources/dotnet/dash0-operator-test-app-dotnet" \
        "test-app-dotnet-ns3" \
        "test-namespace-3" \
        "statefulset" \
        "$test_app_dotnet_image_repository" \
        "$test_app_dotnet_image_tag" \
        "$test_app_dotnet_image_pull_policy"
    fi
  elif [[ "${runtime_under_test:-}" = "python" ]]; then
    build_test_application test-resources/python/flask "$test_app_python_image_repository"
    push_test_application_image "$test_app_python_image_repository"
    helm_install_test_application \
      "test-resources/python/flask/dash0-operator-test-app-python" \
      "test-app-python" \
      "${target_namespace}" \
      "${kind}" \
      "$test_app_python_image_repository" \
      "$test_app_python_image_tag" \
      "$test_app_python_image_pull_policy"
    if [[ "$additional_namespaces" = true ]]; then
      helm_install_test_application \
        "test-resources/python/flask/dash0-operator-test-app-python" \
        "test-app-python-ns2" \
        "test-namespace-2" \
        "daemonset" \
        "$test_app_python_image_repository" \
        "$test_app_python_image_tag" \
        "$test_app_python_image_pull_policy"
      helm_install_test_application \
        "test-resources/python/flask/dash0-operator-test-app-python" \
        "test-app-python-ns3" \
        "test-namespace-3" \
        "statefulset" \
        "$test_app_python_image_repository" \
        "$test_app_python_image_tag" \
        "$test_app_python_image_pull_policy"
    fi
  else
    echo "Error: unknown runtime: $runtime_under_test"
    exit 1
  fi

  finish_step
}

build_test_application() {
  if [[ "${SKIP_TEST_APP_IMAGE_BUILDS:-}" = "true" ]]; then
    return
  fi

  local app_path="$1"
  local image_name="$2"

  pushd "$app_path" > /dev/null
  echo "building container image for application under monitoring ($image_name)"
  docker build . -t "$image_name"
  popd > /dev/null
}

push_test_application_image() {
  if [[ "${PUSH_BUILT_IMAGES:-}" != "true" ]]; then
    return
  fi

  local image_name="$1"
  local image_tag="${2:-latest}"

  echo "pushing test app image [name=$image_name, tag=$image_tag]..."
  docker push "$image_name:$image_tag"
}

helm_install_test_application() {
  local chart_path="$1"
  local release_name="$2"
  local namespace="$3"
  local workload_type="$4"
  local image_repository="$5"
  local image_tag="$6"
  local image_pull_policy="$7"

  local helm_install_command="helm install --namespace $namespace --wait"
  helm_install_command+=" --set ${workload_type}.enabled=true"
  helm_install_command+=" --set image.repository=$image_repository"
  helm_install_command+=" --set image.tag=$image_tag"
  helm_install_command+=" --set image.pullPolicy=$image_pull_policy"
  helm_install_command+=" $release_name"
  helm_install_command+=" $chart_path"
  echo Helm install command for test application:
  echo "$helm_install_command"
  $helm_install_command
}

install_third_party_crds() {
  kubectl apply --server-side -f https://raw.githubusercontent.com/perses/perses-operator/refs/tags/v0.2.0/config/crd/bases/perses.dev_persesdashboards.yaml
  kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.86.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
}

deploy_dash0_api_sync_resources() {
  if [[ "${DEPLOY_SYNTHETIC_CHECK:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Dash0 synthetic check resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/dash0syntheticcheck/dash0syntheticcheck.yaml
    finish_step
  fi
  if [[ "${DEPLOY_VIEW:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Dash0 view resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/dash0view/dash0view.yaml
    finish_step
  fi
  if [[ "${DEPLOY_PERSES_DASHBOARD:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Perses dashboard resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/persesdashboard/persesdashboard.yaml
    finish_step
  fi
  if [[ "${DEPLOY_PROMETHEUS_RULE:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Prometheus rule resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/prometheusrule/prometheusrule.yaml
    finish_step
  fi
}

trim() {
  local var="$*"
  # remove leading whitespace characters
  var="${var#"${var%%[![:space:]]*}"}"
  # remove trailing whitespace characters
  var="${var%"${var##*[![:space:]]}"}"
  printf '%s' "$var"
}

finish_scenario() {
  echo "scenario completed"
  echo
}

finish_step() {
  ((step_counter++))
  echo
  echo
}
