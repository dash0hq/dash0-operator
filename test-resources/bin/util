#!/usr/bin/env bash

# SPDX-FileCopyrightText: Copyright 2024 Dash0 Inc.
# SPDX-License-Identifier: Apache-2.0

###############################################################################
# Utility functions for manual test scenarios.
###############################################################################

operator_namespace=${OPERATOR_NAMESPACE:-operator-namespace}

load_env_file () {
  if [[ ! -f test-resources/.env ]]; then
    echo "error: The file test-resources/.env does not exist. Copy test-resources/.env.template to test-resources/.env and edit it to provide a Dash0 authorization token."
    exit 1
  fi
  source test-resources/.env
}

verify_kubectx() {
  local current_kubectx=$(kubectl config current-context)
  local allowed_contexts_raw="${ALLOWED_KUBECTXS:-}"
  IFS=',' read -r -a allowed_contexts <<< "$allowed_contexts_raw"
  local current_context_is_allowed="false"
  for allowed_context_name in "${allowed_contexts[@]}"; do
    local trimmed=$(trim "$allowed_context_name")
    if [[ "$current_kubectx" = "$trimmed" ]]; then
      current_context_is_allowed="true"
      break
    fi
  done
  if [[ "$current_context_is_allowed" != "true" ]]; then
    echo "error: The current kube context \"$current_kubectx\" is not allowed for testing according to the setting ALLOWED_KUBECTXS (current value: \"$allowed_contexts_raw\"). Please switch to an allowed context or set ALLOWED_KUBECTXS differently in .env. Refusing to run test scenarios/cleanup in context where that is potentially dangerous."
	exit 1
  fi
}

setup_test_environment () {
  test-resources/bin/render-templates.sh
  check_if_kubectx_is_kind_cluster
  determine_container_images
}

ensure_namespace_exists() {
  local target_namespace=${1:-test-namespace}

  if [[ "${target_namespace}" = default  ]]; then
    exit 0
  fi

  if ! kubectl get ns "$target_namespace" &> /dev/null; then
    kubectl create ns "$target_namespace"
  fi
}

finish_step() {
  ((step_counter++))
  echo
  echo
}

is_remote_image() {
  image_name="$1"
  if [[ $image_name = */* ]]; then
    printf "true"
  else
    printf "false"
  fi
}

determine_container_images() {
  # The defaults when using the Helm chart from local sources are:
  # - use the locally built image with tag latest
  # - use pull policy "Never"
  # The defaults when using a remote helm chart are:
  # - default to not setting an explicit image repository or tag
  # - default to not setting a pull policy, instead let Kubernetes use the default pull policy
  controller_img_repository="operator-controller"
  controller_img_tag="latest"
  controller_img_digest=""
  controller_img_pull_policy="Never"
  instrumentation_img_repository="instrumentation"
  instrumentation_img_tag="latest"
  instrumentation_img_digest=""
  instrumentation_img_pull_policy="Never"
  collector_img_repository="collector"
  collector_img_tag="latest"
  collector_img_digest=""
  collector_img_pull_policy="Never"
  configuration_reloader_img_repository="configuration-reloader"
  configuration_reloader_img_tag="latest"
  configuration_reloader_img_digest=""
  configuration_reloader_img_pull_policy="Never"
  filelog_offset_sync_img_repository="filelog-offset-sync"
  filelog_offset_sync_img_tag="latest"
  filelog_offset_sync_img_digest=""
  filelog_offset_sync_img_pull_policy="Never"

  operator_helm_chart=${OPERATOR_HELM_CHART:-helm-chart/dash0-operator}

  is_local_helm_chart="true"
  if [[ "$operator_helm_chart" != "helm-chart/dash0-operator" ]]; then
    echo "Looks like a remote helm chart or an installed Helm chart is used."
    is_local_helm_chart="false"

    controller_img_repository=""
    controller_img_tag=""
    controller_img_pull_policy=""
    instrumentation_img_repository=""
    instrumentation_img_tag=""
    instrumentation_img_pull_policy=""
    collector_img_repository=""
    collector_img_tag=""
    collector_img_pull_policy=""
    configuration_reloader_img_repository=""
    configuration_reloader_img_tag=""
    configuration_reloader_img_pull_policy=""
    filelog_offset_sync_img_repository=""
    filelog_offset_sync_img_tag=""
    filelog_offset_sync_img_pull_policy=""
  fi

  if [[ -n "${CONTROLLER_IMG_REPOSITORY:-}" ]]; then
    controller_img_repository="$CONTROLLER_IMG_REPOSITORY"
  fi
  if [[ -n "${CONTROLLER_IMG_TAG:-}" ]]; then
    controller_img_tag="$CONTROLLER_IMG_TAG"
  fi
  if [[ -n "${CONTROLLER_IMG_DIGEST:-}" ]]; then
    controller_img_digest="$CONTROLLER_IMG_DIGEST"
  fi
  if [[ -n "${CONTROLLER_IMG_PULL_POLICY:-}" ]]; then
    controller_img_pull_policy="$CONTROLLER_IMG_PULL_POLICY"
  fi

  if [[ -n "${INSTRUMENTATION_IMG_REPOSITORY:-}" ]]; then
    instrumentation_img_repository="$INSTRUMENTATION_IMG_REPOSITORY"
  fi
  if [[ -n "${INSTRUMENTATION_IMG_TAG:-}" ]]; then
    instrumentation_img_tag="$INSTRUMENTATION_IMG_TAG"
  fi
  if [[ -n "${INSTRUMENTATION_IMG_DIGEST:-}" ]]; then
    instrumentation_img_digest="$INSTRUMENTATION_IMG_DIGEST"
  fi
  if [[ -n "${INSTRUMENTATION_IMG_PULL_POLICY:-}" ]]; then
    instrumentation_img_pull_policy="$INSTRUMENTATION_IMG_PULL_POLICY"
  fi

  if [[ -n "${COLLECTOR_IMG_REPOSITORY:-}" ]]; then
    collector_img_repository="$COLLECTOR_IMG_REPOSITORY"
  fi
  if [[ -n "${COLLECTOR_IMG_TAG:-}" ]]; then
    collector_img_tag="$COLLECTOR_IMG_TAG"
  fi
  if [[ -n "${COLLECTOR_IMG_DIGEST:-}" ]]; then
    collector_img_digest="$COLLECTOR_IMG_DIGEST"
  fi
  if [[ -n "${COLLECTOR_IMG_PULL_POLICY:-}" ]]; then
    collector_img_pull_policy="$COLLECTOR_IMG_PULL_POLICY"
  fi

  if [[ -n "${CONFIGURATION_RELOADER_IMG_REPOSITORY:-}" ]]; then
    configuration_reloader_img_repository="$CONFIGURATION_RELOADER_IMG_REPOSITORY"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMG_TAG:-}" ]]; then
    configuration_reloader_img_tag="$CONFIGURATION_RELOADER_IMG_TAG"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMG_DIGEST:-}" ]]; then
    configuration_reloader_img_digest="$CONFIGURATION_RELOADER_IMG_DIGEST"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMG_PULL_POLICY:-}" ]]; then
    configuration_reloader_img_pull_policy="$CONFIGURATION_RELOADER_IMG_PULL_POLICY"
  fi

  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_REPOSITORY:-}" ]]; then
    filelog_offset_sync_img_repository="$FILELOG_OFFSET_SYNC_IMG_REPOSITORY"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_TAG:-}" ]]; then
    filelog_offset_sync_img_tag="$FILELOG_OFFSET_SYNC_IMG_TAG"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_DIGEST:-}" ]]; then
    filelog_offset_sync_img_digest="$FILELOG_OFFSET_SYNC_IMG_DIGEST"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_PULL_POLICY:-}" ]]; then
    filelog_offset_sync_img_pull_policy="$FILELOG_OFFSET_SYNC_IMG_PULL_POLICY"
  fi

  if [[ "$is_local_helm_chart" = "true" ]]; then
    # support using the local helm chart with remote images
    if [[ $(is_remote_image "$controller_img_repository") = "true" ]]; then
      controller_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$instrumentation_img_repository") = "true" ]]; then
      instrumentation_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$collector_img_repository") = "true" ]]; then
      collector_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$configuration_reloader_img_repository") = "true" ]]; then
      configuration_reloader_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$filelog_offset_sync_img_repository") = "true" ]]; then
      filelog_offset_sync_img_pull_policy=""
    fi
  fi
}

build_all_images() {
  CONTROLLER_IMG_REPOSITORY="$controller_img_repository" \
    CONTROLLER_IMG_TAG="$controller_img_tag" \
    INSTRUMENTATION_IMG_REPOSITORY="$instrumentation_img_repository" \
    INSTRUMENTATION_IMG_TAG="$instrumentation_img_tag" \
    COLLECTOR_IMG_REPOSITORY="$collector_img_repository" \
    CONFIGURATION_RELOADER_IMG_REPOSITORY="$configuration_reloader_img_repository" \
    CONFIGURATION_RELOADER_IMG_TAG="$configuration_reloader_img_tag" \
    FILELOG_OFFSET_SYNC_IMG_REPOSITORY=$filelog_offset_sync_img_repository \
    FILELOG_OFFSET_SYNC_IMG_TAG="$filelog_offset_sync_img_tag" \
    make docker-build

  if [[ "$is_kind_cluster" = "true" ]]; then
    echo loading images into Kind cluster
    kind load docker-image \
      --name $cluster \
      operator-controller:latest \
      instrumentation:latest \
      collector:latest \
      configuration-reloader:latest \
      filelog-offset-sync:latest
  fi
}

check_if_kubectx_is_kind_cluster() {
  is_kind_cluster="false"
  if command -v kind >/dev/null 2>&1; then
    # kind is installed, check if the current kube context is a kind cluster
    current_kubectx=$(kubectl config current-context)
    kind_clusters=$(kind get clusters 2>&1)
    if [[ "$kind_clusters" = "No kind clusters found." ]]; then
      return
    fi
    while IFS= read -r cluster; do
      if [[ "$current_kubectx" = "kind-$cluster" ]]; then
        is_kind_cluster="true"
        return
      fi
    done <<< "$kind_clusters"
  fi
}

deploy_filelog_offsets_pvc() {
  echo "STEP $step_counter: install filelog offset persistent volume claim"
  if [[ "$is_kind_cluster" = "true" ]]; then
    kubectl patch persistentvolume offset-storage-volume --type=json -p='[{"op": "remove", "path": "/spec/claimRef"}]j' || true
    kubectl apply -f test-resources/customresources/filelogoffsetpvc/offset-storage-pvc-kind.yaml
  else
    kubectl apply -f test-resources/customresources/filelogoffsetpvc/offset-storage-pvc-docker.yaml
  fi
}

deploy_via_helm() {
  run_helm install
}

update_via_helm() {
  run_helm upgrade
}

run_helm() {
  deploy_otlp_sink_if_requested

  local action=${1:-install}
  local helm_install_command="helm $action --namespace $operator_namespace"
  if [[ -n "${OPERATOR_HELM_CHART_VERSION:-}" ]]; then
    helm_install_command+=" --version $OPERATOR_HELM_CHART_VERSION"
  fi
  if [[ -e test-resources/bin/extra-values.yaml ]]; then
    helm_install_command+=" --values test-resources/bin/extra-values.yaml"
  fi
  helm_install_command+=" --set operator.developmentMode=true"
  if [[ "${OTEL_COLLECTOR_DEBUG_VERBOSITY_DETAILED:-}" = "true" ]]; then
    helm_install_command+=" --set operator.collectors.debugVerbosityDetailed=true"
  fi
  if [[ -n "${OTEL_COLLECTOR_SEND_BATCH_MAX_SIZE:-}" ]]; then
    helm_install_command+=" --set operator.collectors.sendBatchMaxSize=$OTEL_COLLECTOR_SEND_BATCH_MAX_SIZE"
  fi
  if [[ "${FILELOG_OFFSETS_PVC:-}" = "true" ]]; then
    helm_install_command+=" --values test-resources/bin/filelog-offset-volume-pvc-values.yaml"
  elif [[ "${FILELOG_OFFSETS_HOST_PATH_VOLUME:-}" = "true" ]]; then
    helm_install_command+=" --values test-resources/bin/filelog-offset-volume-hostpath-values.yaml"
  fi

  if [[ -n "$controller_img_repository" ]]; then
    helm_install_command+=" --set operator.image.repository=$controller_img_repository"
  fi
  if [[ -n "$controller_img_tag" ]]; then
    helm_install_command+=" --set operator.image.tag=$controller_img_tag"
  fi
  if [[ -n "$controller_img_digest" ]]; then
    helm_install_command+=" --set operator.image.digest=$controller_img_digest"
  fi
  if [[ -n "$controller_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.image.pullPolicy=$controller_img_pull_policy"
  fi

  if [[ -n "$instrumentation_img_repository" ]]; then
    helm_install_command+=" --set operator.initContainerImage.repository=$instrumentation_img_repository"
  fi
  if [[ -n "$instrumentation_img_tag" ]]; then
    helm_install_command+=" --set operator.initContainerImage.tag=$instrumentation_img_tag"
  fi
  if [[ -n "$instrumentation_img_digest" ]]; then
    helm_install_command+=" --set operator.initContainerImage.digest=$instrumentation_img_digest"
  fi
  if [[ -n "$instrumentation_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.initContainerImage.pullPolicy=$instrumentation_img_pull_policy"
  fi

  if [[ -n "$collector_img_repository" ]]; then
    helm_install_command+=" --set operator.collectorImage.repository=$collector_img_repository"
  fi
  if [[ -n "$collector_img_tag" ]]; then
    helm_install_command+=" --set operator.collectorImage.tag=$collector_img_tag"
  fi
  if [[ -n "$collector_img_digest" ]]; then
    helm_install_command+=" --set operator.collectorImage.digest=$collector_img_digest"
  fi
  if [[ -n "$collector_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.collectorImage.pullPolicy=$collector_img_pull_policy"
  fi

  if [[ -n "$configuration_reloader_img_repository" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.repository=$configuration_reloader_img_repository"
  fi
  if [[ -n "$configuration_reloader_img_tag" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.tag=$configuration_reloader_img_tag"
  fi
  if [[ -n "$configuration_reloader_img_digest" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.digest=$configuration_reloader_img_digest"
  fi
  if [[ -n "$configuration_reloader_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.pullPolicy=$configuration_reloader_img_pull_policy"
  fi

  if [[ -n "$filelog_offset_sync_img_repository" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.repository=$filelog_offset_sync_img_repository"
  fi
  if [[ -n "$filelog_offset_sync_img_tag" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.tag=$filelog_offset_sync_img_tag"
  fi
  if [[ -n "$filelog_offset_sync_img_digest" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.digest=$filelog_offset_sync_img_digest"
  fi
  if [[ -n "$filelog_offset_sync_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.pullPolicy=$filelog_offset_sync_img_pull_policy"
  fi

  # Deploy an operator configuration right away.
  if [[ "${DEPLOY_OPERATOR_CONFIGURATION_VIA_HELM:-}" != "false" ]]; then
    if [[ "${USE_OTLP_SINK:-}" = "true" ]]; then
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=http://otlp-sink.otlp-sink.svc.cluster.local:4317"
      helm_install_command+=" --set operator.dash0Export.token=dummy-token"
      # Note: API synchronization (Perses dashboards, Prometheus rules) are not supported when using the OTLP sink.
    else
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=${DASH0_INGRESS_ENDPOINT}"
      if [[ "${USE_TOKEN:-}" = true ]]; then
        helm_install_command+=" --set operator.dash0Export.token=${DASH0_AUTHORIZATION_TOKEN}"
      else
        helm_install_command+=" --set operator.dash0Export.secretRef.name=dash0-authorization-secret"
        helm_install_command+=" --set operator.dash0Export.secretRef.key=token"
      fi
      helm_install_command+=" --set operator.dash0Export.apiEndpoint=${DASH0_API_ENDPOINT}"
    fi
    if [[ -n "${OPERATOR_CONFIGURATION_VIA_HELM_DATASET:-}" ]]; then
      helm_install_command+=" --set operator.dash0Export.dataset=$OPERATOR_CONFIGURATION_VIA_HELM_DATASET"
    fi
    if [[ "${SELF_MONITORING_ENABLED:-}" = "false" ]]; then
      helm_install_command+=" --set operator.selfMonitoringEnabled=false"
    fi
    if [[ "${KUBERNETES_INFRASTRUCTURE_METRICS_COLLECTION_ENABLED:-}" = "false" ]]; then
      helm_install_command+=" --set operator.kubernetesInfrastructureMetricsCollectionEnabled=false"
    fi
    helm_install_command+=" --set operator.clusterName=$(kubectl config current-context)"
  fi
  if [[ -n "${OPERATOR_REPLICAS:-}" ]]; then
    helm_install_command+=" --set operator.replicaCount=${OPERATOR_REPLICAS:-1}"
  fi

  helm_install_command+=" dash0-operator"
  helm_install_command+=" ${OPERATOR_HELM_CHART:-helm-chart/dash0-operator}"
  echo Helm install command:
  echo "$helm_install_command"
  $helm_install_command

  wait_for_operator_manager_and_webhook
}

wait_for_operator_manager_and_webhook() {
  echo "waiting for the operator deployment to become available..."
  kubectl wait deployment.apps/dash0-operator-controller --for condition=Available --namespace "$operator_namespace" --timeout 30s

  echo "waiting for the operator's webhook endpoint to become available..."
  local webhook_available="false"
  for ((i=0; i<=100; i++)); do
    local endpoints_output=$(kubectl get endpoints --namespace "$operator_namespace" dash0-operator-webhook-service)
    local expected_endpoint_regex='dash0-operator-webhook-service.+:9443'
    if [[ "$endpoints_output" =~ $expected_endpoint_regex ]]; then
      webhook_available="true"
      break;
    fi
    sleep 0.3
  done
  if [[ "$webhook_available" != "true" ]]; then
    echo "The operator's webhook endpoint never became available."
    exit 1
  fi

  # We deploy an operator configuration at startup via operator.dash0Export.enabled=true, wait for that resource to
  # become available as well.
  if [[ "${DEPLOY_OPERATOR_CONFIGURATION_VIA_HELM:-}" != "false" ]]; then
    echo "waiting for the automatically created operator configuration resource to become available"
    for ((i=0; i<=20; i++)); do
      # wait until the resource has been created
      if kubectl get dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-auto-resource; then
	     break;
	  fi
	  sleep 1
	done
	# wait until the resource has been reconciled and is marked as available
	kubectl wait dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-auto-resource --for condition=Available --timeout 30s
  fi
}

deploy_otlp_sink_if_requested() {
  if [[ "${USE_OTLP_SINK:-}" != "true" ]]; then
	  return
  fi

  echo "deploying otlp-sink"
  tmpfile=$(mktemp "${TMPDIR}otlp-sink-yaml.XXXX")
  trap 'rm -f "$tmpfile"' EXIT
  local otlp_sink_dir="$(realpath "$(pwd)")/test-resources/e2e/volumes/otlp-sink"
  sed "s|/tmp/telemetry|$otlp_sink_dir|g" test-resources/otlp-sink/otlp-sink.yaml > "$tmpfile"

	mkdir -p "$otlp_sink_dir"
	echo "deleting old OTLP sink old telemetry dump files"
	rm -rf "$otlp_sink_dir/traces.jsonl"
	rm -rf "$otlp_sink_dir/metrics.jsonl"
	rm -rf "$otlp_sink_dir/logs.jsonl"
	echo "creating OTLP sink old telemetry dump files"
	touch "$otlp_sink_dir/traces.jsonl"
	touch "$otlp_sink_dir/metrics.jsonl"
	touch "$otlp_sink_dir/logs.jsonl"

	kubectl apply -f "$tmpfile"
	kubectl rollout status \
	  deployment \
	  otlp-sink \
	  --namespace otlp-sink \
	  --timeout 1m
}

install_operator_configuration_resource() {
  if [[ "${USE_OTLP_SINK:-}" = "true" ]]; then
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.otlpsink.yaml
  elif [[ "${USE_TOKEN:-}" = true ]]; then
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.token.yaml
  else
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.secret.yaml
  fi

  echo "waiting for the operator configuration resource to become available"
  kubectl wait dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-resource --for condition=Available

  # Deploying the operator configuration resource might result in a restart of the operator manager deployment pod,
  # hence we check again that both are up and running.
  wait_for_operator_manager_and_webhook
}

install_monitoring_resource() {
  local additional_namespaces=${1:-false}
  kubectl apply -n ${target_namespace} -f test-resources/customresources/dash0monitoring/dash0monitoring.yaml
  if [[ "$additional_namespaces" = "true" ]]; then
    kubectl apply -n test-namespace-2 -f test-resources/customresources/dash0monitoring/dash0monitoring-2.yaml
    kubectl apply -n test-namespace-3 -f test-resources/customresources/dash0monitoring/dash0monitoring-3.yaml
  fi

  echo "waiting for the monitoring resource to become available"
  kubectl wait --namespace ${target_namespace} dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
  if [[ "$additional_namespaces" = "true" ]]; then
    kubectl wait --namespace test-namespace-2 dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
  fi
}

deploy_application_under_monitoring() {
  if [[ "${DEPLOY_APPLICATION_UNDER_MONITORING:-}" != "false" ]]; then
    echo "STEP $step_counter: deploy application under monitoring"
    runtime_under_test="${1:-nodejs}"
    if [[ "${runtime_under_test:-}" = "nodejs" ]]; then
      test-resources/node.js/express/deploy.sh "${target_namespace}" "${kind}"
      if [[ "$additional_namespaces" = true ]]; then
        test-resources/node.js/express/deploy.sh test-namespace-2 daemonset
        test-resources/node.js/express/deploy.sh test-namespace-3 statefulset
      fi
    elif [[ "${runtime_under_test:-}" = "jvm" ]]; then
      pushd test-resources/jvm/spring-boot > /dev/null
      kubectl apply -n "${target_namespace}" -f "${kind}".yaml
      if [[ "$additional_namespaces" = true ]]; then
        kubectl apply -n test-namespace-2 -f daemonset.yaml
        kubectl apply -n test-namespace-3 -f statefulset.yaml
      fi
      popd > /dev/null
    else
      echo "Error: unknown runtime: $runtime_under_test"
      exit 1
    fi
    finish_step
  fi
}

install_third_party_crds() {
  kubectl apply --server-side -f https://raw.githubusercontent.com/perses/perses-operator/refs/tags/v0.1.10/config/crd/bases/perses.dev_persesdashboards.yaml

  kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.82.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
}

install_third_party_resources() {
  if [[ "${DEPLOY_PERSES_DASHBOARD:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Perses dashboard resource to namespace ${target_namespace}"
    kubectl apply -n ${target_namespace} -f test-resources/customresources/persesdashboard/persesdashboard.yaml
    finish_step
  fi
  if [[ "${DEPLOY_PROMETHEUS_RULE:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Prometheus rule resource to namespace ${target_namespace}"
    kubectl apply -n ${target_namespace} -f test-resources/customresources/prometheusrule/prometheusrule.yaml
    finish_step
  fi
}

trim() {
  local var="$*"
  # remove leading whitespace characters
  var="${var#"${var%%[![:space:]]*}"}"
  # remove trailing whitespace characters
  var="${var%"${var##*[![:space:]]}"}"
  printf '%s' "$var"
}

