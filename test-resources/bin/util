#!/usr/bin/env bash

# SPDX-FileCopyrightText: Copyright 2024 Dash0 Inc.
# SPDX-License-Identifier: Apache-2.0

###############################################################################
# Utility functions for manual test scenarios.
###############################################################################

load_env_file () {
  if [[ ! -f test-resources/.env ]]; then
    echo "error: The file test-resources/.env does not exist. Copy test-resources/.env.template to test-resources/.env and edit it to provide a Dash0 authorization token."
    exit 1
  fi
  source test-resources/.env
}

verify_kubectx() {
  local current_kubectx=$(kubectl config current-context)
  local allowed_contexts_raw="${ALLOWED_KUBECTXS:-}"
  IFS=',' read -r -a allowed_contexts <<< "$allowed_contexts_raw"
  local current_context_is_allowed=false
  for allowed_context_name in "${allowed_contexts[@]}"; do
    local trimmed=$(trim "$allowed_context_name")
    if [[ "$current_kubectx" == "$trimmed" ]]; then
      current_context_is_allowed=true
      break
    fi
  done
  if [[ "$current_context_is_allowed" != true ]]; then
    echo "error: The current kube context \"$current_kubectx\" is not allowed for testing according to the setting ALLOWED_KUBECTXS (current value: \"$allowed_contexts_raw\"). Please switch to an allowed context or set ALLOWED_KUBECTXS differently in .env. Refusing to run test scenarios/cleanup in context where that is potentially dangerous."
	exit 1
  fi
}

setup_test_environment () {
  test-resources/bin/render-templates.sh
  check_if_kubectx_is_kind_cluster
}

ensure_namespace_exists() {
  local target_namespace=${1:-test-namespace}

  if [[ "${target_namespace}" == default  ]]; then
    exit 0
  fi

  if ! kubectl get ns "$target_namespace" &> /dev/null; then
    kubectl create ns "$target_namespace"
  fi
}

finish_step() {
  ((step_counter++))
  echo
  echo
}

build_all_images() {
  make docker-build

  if [[ "$is_kind_cluster" == true ]]; then
    echo loading images into Kind cluster
    kind load docker-image \
      --name $cluster \
      operator-controller:latest \
      instrumentation:latest \
      collector:latest \
      configuration-reloader:latest \
      filelog-offset-synch:latest
  fi
}

check_if_kubectx_is_kind_cluster() {
  is_kind_cluster=false
  if command -v kind >/dev/null 2>&1; then
    # kind is installed, check if the current kube context is a kind cluster
    current_kubectx=$(kubectl config current-context)
    kind_clusters=$(kind get clusters 2>&1)
    if [[ "$kind_clusters" == "No kind clusters found." ]]; then
      return
    fi
    while IFS= read -r cluster; do
      if [[ "$current_kubectx" == "kind-$cluster" ]]; then
        is_kind_cluster=true
        return
      fi
    done <<< "$kind_clusters"
  fi
}

deploy_via_helm() {
  run_helm install
}

update_via_helm() {
  run_helm upgrade
}

run_helm() {
  deploy_otlp_sink_if_requested

  local action=${1:-install}
  local helm_install_command="helm $action --namespace dash0-system"
  if [[ -n "${OPERATOR_HELM_CHART_VERSION:-}" ]]; then
    helm_install_command+=" --version $OPERATOR_HELM_CHART_VERSION"
  fi
  helm_install_command+=" --set operator.developmentMode=true"
  if ! has_been_set_to_empty_string "CONTROLLER_IMG_REPOSITORY"; then
    helm_install_command+=" --set operator.image.repository=${CONTROLLER_IMG_REPOSITORY:-operator-controller}"
  fi
  if ! has_been_set_to_empty_string "CONTROLLER_IMG_TAG"; then
    helm_install_command+=" --set operator.image.tag=${CONTROLLER_IMG_TAG:-latest}"
  fi
  if [[ -n "${CONTROLLER_IMG_DIGEST:-}" ]]; then
    helm_install_command+=" --set operator.image.digest=$IMG_DIGEST"
  fi
  if ! has_been_set_to_empty_string "CONTROLLER_IMG_PULL_POLICY"; then
    helm_install_command+=" --set operator.image.pullPolicy=${CONTROLLER_IMG_PULL_POLICY:-Never}"
  fi

  if ! has_been_set_to_empty_string "INSTRUMENTATION_IMG_REPOSITORY"; then
    helm_install_command+=" --set operator.initContainerImage.repository=${INSTRUMENTATION_IMG_REPOSITORY:-instrumentation}"
  fi
  if ! has_been_set_to_empty_string "INSTRUMENTATION_IMG_TAG"; then
    helm_install_command+=" --set operator.initContainerImage.tag=${INSTRUMENTATION_IMG_TAG:-latest}"
  fi
  if [[ -n "${INSTRUMENTATION_IMG_DIGEST:-}" ]]; then
    helm_install_command+=" --set operator.initContainerImage.digest=$INSTRUMENTATION_IMG_DIGEST"
  fi
  if ! has_been_set_to_empty_string "INSTRUMENTATION_IMG_PULL_POLICY"; then
    helm_install_command+=" --set operator.initContainerImage.pullPolicy=${INSTRUMENTATION_IMG_PULL_POLICY:-Never}"
  fi

  if ! has_been_set_to_empty_string "COLLECTOR_IMG_REPOSITORY"; then
    helm_install_command+=" --set operator.collectorImage.repository=${COLLECTOR_IMG_REPOSITORY:-collector}"
  fi
  if ! has_been_set_to_empty_string "COLLECTOR_IMG_TAG"; then
    helm_install_command+=" --set operator.collectorImage.tag=${COLLECTOR_IMG_TAG:-latest}"
  fi
  if [[ -n "${COLLECTOR_IMG_DIGEST:-}" ]]; then
    helm_install_command+=" --set operator.collectorImage.digest=$COLLECTOR_IMG_DIGEST"
  fi
  if ! has_been_set_to_empty_string "COLLECTOR_IMG_PULL_POLICY"; then
    helm_install_command+=" --set operator.collectorImage.pullPolicy=${COLLECTOR_IMG_PULL_POLICY:-Never}"
  fi

  if ! has_been_set_to_empty_string "CONFIGURATION_RELOADER_IMG_REPOSITORY"; then
    helm_install_command+=" --set operator.configurationReloaderImage.repository=${CONFIGURATION_RELOADER_IMG_REPOSITORY:-configuration-reloader}"
  fi
  if ! has_been_set_to_empty_string "CONFIGURATION_RELOADER_IMG_TAG"; then
    helm_install_command+=" --set operator.configurationReloaderImage.tag=${CONFIGURATION_RELOADER_IMG_TAG:-latest}"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMG_DIGEST:-}" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.digest=$CONFIGURATION_RELOADER_IMG_DIGEST"
  fi
  if ! has_been_set_to_empty_string "CONFIGURATION_RELOADER_IMG_PULL_POLICY"; then
    helm_install_command+=" --set operator.configurationReloaderImage.pullPolicy=${CONFIGURATION_RELOADER_IMG_PULL_POLICY:-Never}"
  fi

  if ! has_been_set_to_empty_string "FILELOG_OFFSET_SYNCH_IMG_REPOSITORY"; then
    helm_install_command+=" --set operator.filelogOffsetSynchImage.repository=${FILELOG_OFFSET_SYNCH_IMG_REPOSITORY:-filelog-offset-synch}"
  fi
  if ! has_been_set_to_empty_string "FILELOG_OFFSET_SYNCH_IMG_TAG"; then
    helm_install_command+=" --set operator.filelogOffsetSynchImage.tag=${FILELOG_OFFSET_SYNCH_IMG_TAG:-latest}"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNCH_IMG_DIGEST:-}" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSynchImage.digest=$FILELOG_OFFSET_SYNCH_IMG_DIGEST"
  fi
  if ! has_been_set_to_empty_string "FILELOG_OFFSET_SYNCH_IMG_PULL_POLICY"; then
    helm_install_command+=" --set operator.filelogOffsetSynchImage.pullPolicy=${FILELOG_OFFSET_SYNCH_IMG_PULL_POLICY:-Never}"
  fi

  # Deploy an operator configuration right away.
  if [[ "${DEPLOY_OPERATOR_CONFIGURATION_VIA_HELM:-}" != false ]]; then
    if [[ "${USE_OTLP_SINK:-}" == true ]]; then
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=http://otlp-sink.otlp-sink.svc.cluster.local:4317"
      helm_install_command+=" --set operator.dash0Export.token=dummy-token"
      # Note: API synchronization (Perses dashboards, Prometheus rules) are not supported when using the OTLP sink.
    else
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=${DASH0_INGRESS_ENDPOINT}"
      if [[ "${OPERATOR_CONFIGURATION_VIA_HELM_USE_TOKEN:-}" == true ]]; then
        helm_install_command+=" --set operator.dash0Export.token=${DASH0_AUTHORIZATION_TOKEN}"
      else
        helm_install_command+=" --set operator.dash0Export.secretRef.name=dash0-authorization-secret"
        helm_install_command+=" --set operator.dash0Export.secretRef.key=token"
      fi
      helm_install_command+=" --set operator.dash0Export.apiEndpoint=${DASH0_API_ENDPOINT}"
    fi
    if [[ -n "${OPERATOR_CONFIGURATION_VIA_HELM_DATASET:-}" ]]; then
      helm_install_command+=" --set operator.dash0Export.dataset=$OPERATOR_CONFIGURATION_VIA_HELM_DATASET"
    fi
    if [[ "${OPERATOR_CONFIGURATION_VIA_HELM_SELF_MONITORING_ENABLED:-}" == false  ]]; then
      helm_install_command+=" --set operator.selfMonitoringEnabled=false"
    fi
    if [[ "${OPERATOR_CONFIGURATION_VIA_HELM_KUBERNETES_INFRASTRUCTURE_METRICS_COLLECTION_ENABLED:-}" == false  ]]; then
      helm_install_command+=" --set operator.kubernetesInfrastructureMetricsCollectionEnabled=false"
    fi
    helm_install_command+=" --set operator.clusterName=local-operator-test-cluster"
  fi

  helm_install_command+=" dash0-operator"
  helm_install_command+=" ${OPERATOR_HELM_CHART:-helm-chart/dash0-operator}"
  echo Helm install command:
  echo "$helm_install_command"
  $helm_install_command

  wait_for_operator_manager_and_webhook
}


wait_for_operator_manager_and_webhook() {
  echo "waiting for the operator deployment to become available..."
  kubectl wait deployment.apps/dash0-operator-controller --for condition=Available --namespace dash0-system --timeout 30s

  echo "waiting for the operator's webhook endpoint to become available..."
  local webhook_available=false
  for ((i=0; i<=100; i++)); do
    local endpoints_output=$(kubectl get endpoints --namespace dash0-system dash0-operator-webhook-service)
    local expected_endpoint_regex='dash0-operator-webhook-service.+:9443'
    if [[ "$endpoints_output" =~ $expected_endpoint_regex ]]; then
      webhook_available=true
      break;
    fi
    sleep 0.3
  done
  if [[ "$webhook_available" != true ]]; then
    echo "The operator's webhook endpoint never became available."
    exit 1
  fi

  # We deploy an operator configuration at startup via operator.dash0Export.enabled=true, wait for that resource to
  # become available as well.
  if [[ "${DEPLOY_OPERATOR_CONFIGURATION_VIA_HELM:-}" != false ]]; then
    echo "waiting for the automatically created operator configuration resource to become available"
    for ((i=0; i<=20; i++)); do
      # wait until the resource has been created
      if kubectl get dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-auto-resource; then
	     break;
	  fi
	  sleep 1
	done
	# wait until the resource has been reconciled and is marked as available
	kubectl wait dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-auto-resource --for condition=Available --timeout 30s
  fi
}

has_been_set_to_empty_string() {
  local env_var_name=$1
  local value=${!env_var_name:-}
  local is_set=${!env_var_name+is_set}

  if [ -z "$value" ] && [ "$is_set" = "is_set" ]; then
    return 0
  elif [ -n "$value" ]; then
    return 1
  else
    return 1
  fi
}

deploy_otlp_sink_if_requested() {
  if [[ "${USE_OTLP_SINK:-}" != true ]]; then
	  return
  fi

  echo "deploying otlp-sink"
  tmpfile=$(mktemp "${TMPDIR}otlp-sink-yaml.XXXX")
  trap 'rm -f "$tmpfile"' EXIT
  local otlp_sink_dir="$(realpath "$(pwd)")/test-resources/e2e-test-volumes/otlp-sink"
  sed "s|/tmp/telemetry|$otlp_sink_dir|g" test-resources/otlp-sink/otlp-sink.yaml > "$tmpfile"

	kubectl apply -f "$tmpfile"
	kubectl rollout status \
	  deployment \
	  otlp-sink \
	  --namespace otlp-sink \
	  --timeout 1m
}

install_operator_configuration_resource() {
  if [[ "{USE_OTLP_SINK:-}" = "true" ]]; then
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0monitoring.otlpsink.yaml
  else
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.token.yaml
  fi

  echo "waiting for the operator configuration resource to become available"
  kubectl wait dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-resource --for condition=Available

  # Deploying the operator configuration resource might result in a restart of the operator manager deployment pod,
  # hence we check again that both are up and running.
  wait_for_operator_manager_and_webhook
}

install_monitoring_resource() {
  kubectl apply -n ${target_namespace} -f test-resources/customresources/dash0monitoring/dash0monitoring.yaml

  echo "waiting for the monitoring resource to become available"
  kubectl wait --namespace ${target_namespace} dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
}

install_third_party_crds() {
  kubectl apply --server-side -f https://raw.githubusercontent.com/perses/perses-operator/main/config/crd/bases/perses.dev_persesdashboards.yaml
  kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.1/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
}

install_third_party_resources() {
  if [[ "${DEPLOY_PERSES_DASHBOARD:-}" == true ]]; then
    echo "STEP $step_counter: deploy a Perses dashboard resource to namespace ${target_namespace}"
    kubectl apply -n ${target_namespace} -f test-resources/customresources/persesdashboard/persesdashboard.yaml
    finish_step
  fi
  if [[ "${DEPLOY_PROMETHEUS_RULE:-}" == true ]]; then
    echo "STEP $step_counter: deploy a Prometheus rule resource to namespace ${target_namespace}"
    kubectl apply -n ${target_namespace} -f test-resources/customresources/prometheusrule/prometheusrule.yaml
    finish_step
  fi
}

trim() {
  local var="$*"
  # remove leading whitespace characters
  var="${var#"${var%%[![:space:]]*}"}"
  # remove trailing whitespace characters
  var="${var%"${var##*[![:space:]]}"}"
  printf '%s' "$var"
}

