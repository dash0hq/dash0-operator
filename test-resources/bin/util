#!/usr/bin/env bash

# SPDX-FileCopyrightText: Copyright 2024 Dash0 Inc.
# SPDX-License-Identifier: Apache-2.0

###############################################################################
# Utility functions for manual test scenarios.
###############################################################################

operator_namespace=${OPERATOR_NAMESPACE:-operator-namespace}

load_env_file () {
  if [[ ! -f test-resources/.env ]]; then
    echo "error: The file test-resources/.env does not exist. Copy test-resources/.env.template to test-resources/.env and edit it to provide a Dash0 authorization token."
    exit 1
  fi
  source test-resources/.env
}

verify_kubectx() {
  local current_kubectx=$(kubectl config current-context)
  local allowed_contexts_raw="${ALLOWED_KUBECTXS:-}"
  IFS=',' read -r -a allowed_contexts <<< "$allowed_contexts_raw"
  local current_context_is_allowed="false"
  for allowed_context_name in "${allowed_contexts[@]}"; do
    local trimmed=$(trim "$allowed_context_name")
    if [[ "$current_kubectx" = "$trimmed" ]]; then
      current_context_is_allowed="true"
      break
    fi
  done
  if [[ "$current_context_is_allowed" != "true" ]]; then
    echo "error: The current kube context \"$current_kubectx\" is not allowed for testing according to the setting ALLOWED_KUBECTXS (current value: \"$allowed_contexts_raw\"). Please switch to an allowed context or set ALLOWED_KUBECTXS differently in .env. Refusing to run test scenarios/cleanup in context where that is potentially dangerous."
    exit 1
  fi
}

setup_test_environment () {
  if ! docker info > /dev/null 2>&1; then
    echo "This script uses docker, but it looks like Docker is not running. Please start docker and try again."
    exit 1
  fi

  test-resources/bin/render-templates.sh
  check_if_kubectx_is_kind_or_minikube_cluster
  determine_container_images

  install_cert_manager_if_necessary
}

ensure_namespace_exists() {
  local target_namespace=${1:-test-namespace}

  if [[ "${target_namespace}" = default  ]]; then
    exit 0
  fi

  if ! kubectl get ns "$target_namespace" &> /dev/null; then
    kubectl create ns "$target_namespace"
  fi
}

install_cert_manager_if_necessary() {
  if [[ "${USE_CERT_MANAGER:-}" = "true" ]]; then
    if ! kubectl get ns cert-manager &> /dev/null; then
      echo cert-manager namespace not found, deploying cert-manager
      test-resources/cert-manager/deploy.sh
    fi
    operator_webhook_service_name=custom-dash0-operator-webhook-service-name
  fi
}

is_remote_image() {
  image_name="$1"
  if [[ $image_name = */* ]]; then
    printf "true"
  else
    printf "false"
  fi
}

determine_container_images() {
  # The defaults when using the Helm chart from local sources are:
  # - use the locally built image with tag latest
  # - use pull policy "Never"
  # The defaults when using a remote helm chart are:
  # - default to not setting an explicit image repository or tag
  # - default to not setting a pull policy, instead let Kubernetes use the default pull policy
  controller_img_repository="operator-controller"
  controller_img_tag="latest"
  controller_img_digest=""
  controller_img_pull_policy="Never"
  instrumentation_img_repository="instrumentation"
  instrumentation_img_tag="latest"
  instrumentation_img_digest=""
  instrumentation_img_pull_policy="Never"
  collector_img_repository="collector"
  collector_img_tag="latest"
  collector_img_digest=""
  collector_img_pull_policy="Never"
  configuration_reloader_img_repository="configuration-reloader"
  configuration_reloader_img_tag="latest"
  configuration_reloader_img_digest=""
  configuration_reloader_img_pull_policy="Never"
  filelog_offset_sync_img_repository="filelog-offset-sync"
  filelog_offset_sync_img_tag="latest"
  filelog_offset_sync_img_digest=""
  filelog_offset_sync_img_pull_policy="Never"

  operator_helm_chart=${OPERATOR_HELM_CHART:-helm-chart/dash0-operator}

  is_local_helm_chart="true"
  if [[ "$operator_helm_chart" != "helm-chart/dash0-operator" ]]; then
    echo "Looks like a remote helm chart or an installed Helm chart is used."
    is_local_helm_chart="false"

    controller_img_repository=""
    controller_img_tag=""
    controller_img_pull_policy=""
    instrumentation_img_repository=""
    instrumentation_img_tag=""
    instrumentation_img_pull_policy=""
    collector_img_repository=""
    collector_img_tag=""
    collector_img_pull_policy=""
    configuration_reloader_img_repository=""
    configuration_reloader_img_tag=""
    configuration_reloader_img_pull_policy=""
    filelog_offset_sync_img_repository=""
    filelog_offset_sync_img_tag=""
    filelog_offset_sync_img_pull_policy=""
  fi

  if [[ -n "${CONTROLLER_IMG_REPOSITORY:-}" ]]; then
    controller_img_repository="$CONTROLLER_IMG_REPOSITORY"
  fi
  if [[ -n "${CONTROLLER_IMG_TAG:-}" ]]; then
    controller_img_tag="$CONTROLLER_IMG_TAG"
  fi
  if [[ -n "${CONTROLLER_IMG_DIGEST:-}" ]]; then
    controller_img_digest="$CONTROLLER_IMG_DIGEST"
  fi
  if [[ -n "${CONTROLLER_IMG_PULL_POLICY:-}" ]]; then
    controller_img_pull_policy="$CONTROLLER_IMG_PULL_POLICY"
  fi

  if [[ -n "${INSTRUMENTATION_IMG_REPOSITORY:-}" ]]; then
    instrumentation_img_repository="$INSTRUMENTATION_IMG_REPOSITORY"
  fi
  if [[ -n "${INSTRUMENTATION_IMG_TAG:-}" ]]; then
    instrumentation_img_tag="$INSTRUMENTATION_IMG_TAG"
  fi
  if [[ -n "${INSTRUMENTATION_IMG_DIGEST:-}" ]]; then
    instrumentation_img_digest="$INSTRUMENTATION_IMG_DIGEST"
  fi
  if [[ -n "${INSTRUMENTATION_IMG_PULL_POLICY:-}" ]]; then
    instrumentation_img_pull_policy="$INSTRUMENTATION_IMG_PULL_POLICY"
  fi

  if [[ -n "${COLLECTOR_IMG_REPOSITORY:-}" ]]; then
    collector_img_repository="$COLLECTOR_IMG_REPOSITORY"
  fi
  if [[ -n "${COLLECTOR_IMG_TAG:-}" ]]; then
    collector_img_tag="$COLLECTOR_IMG_TAG"
  fi
  if [[ -n "${COLLECTOR_IMG_DIGEST:-}" ]]; then
    collector_img_digest="$COLLECTOR_IMG_DIGEST"
  fi
  if [[ -n "${COLLECTOR_IMG_PULL_POLICY:-}" ]]; then
    collector_img_pull_policy="$COLLECTOR_IMG_PULL_POLICY"
  fi

  if [[ -n "${CONFIGURATION_RELOADER_IMG_REPOSITORY:-}" ]]; then
    configuration_reloader_img_repository="$CONFIGURATION_RELOADER_IMG_REPOSITORY"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMG_TAG:-}" ]]; then
    configuration_reloader_img_tag="$CONFIGURATION_RELOADER_IMG_TAG"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMG_DIGEST:-}" ]]; then
    configuration_reloader_img_digest="$CONFIGURATION_RELOADER_IMG_DIGEST"
  fi
  if [[ -n "${CONFIGURATION_RELOADER_IMG_PULL_POLICY:-}" ]]; then
    configuration_reloader_img_pull_policy="$CONFIGURATION_RELOADER_IMG_PULL_POLICY"
  fi

  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_REPOSITORY:-}" ]]; then
    filelog_offset_sync_img_repository="$FILELOG_OFFSET_SYNC_IMG_REPOSITORY"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_TAG:-}" ]]; then
    filelog_offset_sync_img_tag="$FILELOG_OFFSET_SYNC_IMG_TAG"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_DIGEST:-}" ]]; then
    filelog_offset_sync_img_digest="$FILELOG_OFFSET_SYNC_IMG_DIGEST"
  fi
  if [[ -n "${FILELOG_OFFSET_SYNC_IMG_PULL_POLICY:-}" ]]; then
    filelog_offset_sync_img_pull_policy="$FILELOG_OFFSET_SYNC_IMG_PULL_POLICY"
  fi

  if [[ "$is_local_helm_chart" = "true" ]]; then
    # support using the local helm chart with remote images
    if [[ $(is_remote_image "$controller_img_repository") = "true" ]]; then
      controller_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$instrumentation_img_repository") = "true" ]]; then
      instrumentation_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$collector_img_repository") = "true" ]]; then
      collector_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$configuration_reloader_img_repository") = "true" ]]; then
      configuration_reloader_img_pull_policy=""
    fi
    if [[ $(is_remote_image "$filelog_offset_sync_img_repository") = "true" ]]; then
      filelog_offset_sync_img_pull_policy=""
    fi
  fi
}

build_all_images() {
  CONTROLLER_IMG_REPOSITORY="$controller_img_repository" \
    CONTROLLER_IMG_TAG="$controller_img_tag" \
    INSTRUMENTATION_IMG_REPOSITORY="$instrumentation_img_repository" \
    INSTRUMENTATION_IMG_TAG="$instrumentation_img_tag" \
    COLLECTOR_IMG_REPOSITORY="$collector_img_repository" \
    CONFIGURATION_RELOADER_IMG_REPOSITORY="$configuration_reloader_img_repository" \
    CONFIGURATION_RELOADER_IMG_TAG="$configuration_reloader_img_tag" \
    FILELOG_OFFSET_SYNC_IMG_REPOSITORY=$filelog_offset_sync_img_repository \
    FILELOG_OFFSET_SYNC_IMG_TAG="$filelog_offset_sync_img_tag" \
    make docker-build

  if [[ "$is_kind_cluster" = "true" ]]; then
    echo loading images into Kind cluster
    kind load docker-image \
      --name "$kind_cluster_name" \
      operator-controller:latest \
      instrumentation:latest \
      collector:latest \
      configuration-reloader:latest \
      filelog-offset-sync:latest
  fi
}

check_if_kubectx_is_kind_or_minikube_cluster() {
  check_if_kubectx_is_kind
  check_if_kubectx_is_minikube
}

check_if_kubectx_is_kind() {
  is_kind_cluster="false"
  if command -v kind >/dev/null 2>&1; then
    # kind is installed, check if the current kube context is a kind cluster
    current_kubectx=$(kubectl config current-context)
    kind_clusters=$(kind get clusters 2>&1)
    if [[ "$kind_clusters" = "No kind clusters found." ]]; then
      return
    fi
    while IFS= read -r cluster; do
      if [[ "$current_kubectx" = "kind-$cluster" ]]; then
        is_kind_cluster="true"
        kind_cluster_name="$cluster"
        return
      fi
    done <<< "$kind_clusters"
  fi
}

check_if_kubectx_is_minikube() {
  is_minikube_cluster="false"
  if command -v minikube >/dev/null 2>&1; then
    # minikube is installed, check if the current kube context is a minikube cluster
    current_kubectx=$(kubectl config current-context)
    minikube_clusters=$(minikube profile list --output=json | jq -r '.valid[].Name')
    while IFS= read -r cluster; do
      if [[ "$current_kubectx" = "$cluster" ]]; then
        echo 'Detected minikube cluster, will run `eval $(minikube docker-env)`'
        is_minikube_cluster="true"
        eval $(minikube docker-env)
        return
      fi
    done <<< "$minikube_clusters"
  fi
}

deploy_filelog_offsets_pvc() {
  echo "STEP $step_counter: install filelog offset persistent volume claim"
  if [[ "$is_kind_cluster" = "true" ]]; then
    kubectl patch persistentvolume offset-storage-volume --type=json -p='[{"op": "remove", "path": "/spec/claimRef"}]j' || true
    kubectl apply -f test-resources/customresources/filelogoffsetpvc/offset-storage-pvc-kind.yaml
  else
    kubectl apply -f test-resources/customresources/filelogoffsetpvc/offset-storage-pvc-docker.yaml
  fi
}

deploy_additional_resources() {
  deploy_priority_classes
  deploy_additional_cert_manager_resources
}

deploy_priority_classes() {
  echo "STEP $step_counter: deploying priority classes"
  kubectl apply -f test-resources/customresources/priorityclass/priorityclasses.yaml
  finish_step
}

deploy_additional_cert_manager_resources() {
  if [[ "${USE_CERT_MANAGER:-}" = "true" ]]; then
    echo "STEP $step_counter: install certificate and issuer for cert-manager"
    kubectl apply -n "$operator_namespace" -f test-resources/cert-manager/certificate-and-issuer.yaml
    finish_step
  fi
}

deploy_via_helm() {
  run_helm install
}

update_via_helm() {
  run_helm upgrade
}

run_helm() {
  deploy_otlp_sink_if_requested

  local action=${1:-install}
  local helm_install_command="helm $action --wait --namespace $operator_namespace"
  if [[ -n "${OPERATOR_HELM_CHART_VERSION:-}" ]]; then
    helm_install_command+=" --version $OPERATOR_HELM_CHART_VERSION"
  fi
  if [[ -e test-resources/bin/extra-values.yaml ]]; then
    helm_install_command+=" --values test-resources/bin/extra-values.yaml"
  fi
  if [[ "${USE_CERT_MANAGER:-}" = "true" ]]; then
     helm_install_command+=" --values test-resources/cert-manager/helm-values.yaml"
  fi
  helm_install_command+=" --set operator.developmentMode=true"
  if [[ "${OTEL_COLLECTOR_DEBUG_VERBOSITY_DETAILED:-}" = "true" ]]; then
    helm_install_command+=" --set operator.collectors.debugVerbosityDetailed=true"
  fi
  if [[ -n "${OTEL_COLLECTOR_SEND_BATCH_MAX_SIZE:-}" ]]; then
    helm_install_command+=" --set operator.collectors.sendBatchMaxSize=$OTEL_COLLECTOR_SEND_BATCH_MAX_SIZE"
  fi
  if [[ "${FILELOG_OFFSETS_PVC:-}" = "true" ]]; then
    helm_install_command+=" --values test-resources/bin/filelog-offset-volume-pvc-values.yaml"
  elif [[ "${FILELOG_OFFSETS_HOST_PATH_VOLUME:-}" = "true" ]]; then
    helm_install_command+=" --values test-resources/bin/filelog-offset-volume-hostpath-values.yaml"
  fi

  if [[ -n "$controller_img_repository" ]]; then
    helm_install_command+=" --set operator.image.repository=$controller_img_repository"
  fi
  if [[ -n "$controller_img_tag" ]]; then
    helm_install_command+=" --set operator.image.tag=$controller_img_tag"
  fi
  if [[ -n "$controller_img_digest" ]]; then
    helm_install_command+=" --set operator.image.digest=$controller_img_digest"
  fi
  if [[ -n "$controller_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.image.pullPolicy=$controller_img_pull_policy"
  fi

  if [[ -n "$instrumentation_img_repository" ]]; then
    helm_install_command+=" --set operator.initContainerImage.repository=$instrumentation_img_repository"
  fi
  if [[ -n "$instrumentation_img_tag" ]]; then
    helm_install_command+=" --set operator.initContainerImage.tag=$instrumentation_img_tag"
  fi
  if [[ -n "$instrumentation_img_digest" ]]; then
    helm_install_command+=" --set operator.initContainerImage.digest=$instrumentation_img_digest"
  fi
  if [[ -n "$instrumentation_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.initContainerImage.pullPolicy=$instrumentation_img_pull_policy"
  fi

  if [[ -n "$collector_img_repository" ]]; then
    helm_install_command+=" --set operator.collectorImage.repository=$collector_img_repository"
  fi
  if [[ -n "$collector_img_tag" ]]; then
    helm_install_command+=" --set operator.collectorImage.tag=$collector_img_tag"
  fi
  if [[ -n "$collector_img_digest" ]]; then
    helm_install_command+=" --set operator.collectorImage.digest=$collector_img_digest"
  fi
  if [[ -n "$collector_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.collectorImage.pullPolicy=$collector_img_pull_policy"
  fi

  if [[ -n "$configuration_reloader_img_repository" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.repository=$configuration_reloader_img_repository"
  fi
  if [[ -n "$configuration_reloader_img_tag" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.tag=$configuration_reloader_img_tag"
  fi
  if [[ -n "$configuration_reloader_img_digest" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.digest=$configuration_reloader_img_digest"
  fi
  if [[ -n "$configuration_reloader_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.configurationReloaderImage.pullPolicy=$configuration_reloader_img_pull_policy"
  fi

  if [[ -n "$filelog_offset_sync_img_repository" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.repository=$filelog_offset_sync_img_repository"
  fi
  if [[ -n "$filelog_offset_sync_img_tag" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.tag=$filelog_offset_sync_img_tag"
  fi
  if [[ -n "$filelog_offset_sync_img_digest" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.digest=$filelog_offset_sync_img_digest"
  fi
  if [[ -n "$filelog_offset_sync_img_pull_policy" ]]; then
    helm_install_command+=" --set operator.filelogOffsetSyncImage.pullPolicy=$filelog_offset_sync_img_pull_policy"
  fi

  # Deploy an operator configuration right away.
  if [[ "${DEPLOY_OPERATOR_CONFIGURATION_VIA_HELM:-}" != "false" ]]; then
    if [[ "${USE_OTLP_SINK:-}" = "true" ]]; then
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=http://otlp-sink.otlp-sink.svc.cluster.local:4317"
      helm_install_command+=" --set operator.dash0Export.token=dummy-token"
      # Note: API synchronization (Perses dashboards, Prometheus rules) are not supported when using the OTLP sink.
    else
      helm_install_command+=" --set operator.dash0Export.enabled=true"
      helm_install_command+=" --set operator.dash0Export.endpoint=${DASH0_INGRESS_ENDPOINT}"
      if [[ "${USE_TOKEN:-}" = true ]]; then
        helm_install_command+=" --set operator.dash0Export.token=${DASH0_AUTHORIZATION_TOKEN}"
      else
        helm_install_command+=" --set operator.dash0Export.secretRef.name=dash0-authorization-secret"
        helm_install_command+=" --set operator.dash0Export.secretRef.key=token"
      fi
      helm_install_command+=" --set operator.dash0Export.apiEndpoint=${DASH0_API_ENDPOINT}"
    fi
    if [[ -n "${OPERATOR_CONFIGURATION_VIA_HELM_DATASET:-}" ]]; then
      helm_install_command+=" --set operator.dash0Export.dataset=$OPERATOR_CONFIGURATION_VIA_HELM_DATASET"
    fi
    if [[ "${SELF_MONITORING_ENABLED:-}" = "false" ]]; then
      helm_install_command+=" --set operator.selfMonitoringEnabled=false"
    fi
    if [[ "${KUBERNETES_INFRASTRUCTURE_METRICS_COLLECTION_ENABLED:-}" = "false" ]]; then
      helm_install_command+=" --set operator.kubernetesInfrastructureMetricsCollectionEnabled=false"
    fi
    helm_install_command+=" --set operator.clusterName=$(kubectl config current-context)"
  fi
  if [[ -n "${OPERATOR_REPLICAS:-}" ]]; then
    helm_install_command+=" --set operator.replicaCount=${OPERATOR_REPLICAS:-1}"
  fi

  helm_install_command+=" dash0-operator"
  helm_install_command+=" ${OPERATOR_HELM_CHART:-helm-chart/dash0-operator}"
  echo Helm install command:
  echo "$helm_install_command"
  # helm install --wait can take a couple of seconds, since the readiness check and the Helm post-install hook make sure
  # that the operator deployment is actually ready, and a monitoring resource can be deployed immediately afterwards.
  # Executing the command with `time` to get some feedback about the duration.
  time $helm_install_command
}

deploy_otlp_sink_if_requested() {
  if [[ "${USE_OTLP_SINK:-}" != "true" ]]; then
    return
  fi

  echo "deploying otlp-sink"
  tmpfile=$(mktemp "${TMPDIR}otlp-sink-yaml.XXXX")
  trap 'rm -f "$tmpfile"' EXIT
  local otlp_sink_dir="$(realpath "$(pwd)")/test-resources/e2e/volumes/otlp-sink"
  sed "s|/tmp/telemetry|$otlp_sink_dir|g" test-resources/otlp-sink/otlp-sink.yaml > "$tmpfile"

  mkdir -p "$otlp_sink_dir"
  echo "deleting old OTLP sink old telemetry dump files"
  rm -rf "$otlp_sink_dir/traces.jsonl"
  rm -rf "$otlp_sink_dir/metrics.jsonl"
  rm -rf "$otlp_sink_dir/logs.jsonl"
  echo "creating OTLP sink old telemetry dump files"
  touch "$otlp_sink_dir/traces.jsonl"
  touch "$otlp_sink_dir/metrics.jsonl"
  touch "$otlp_sink_dir/logs.jsonl"

  kubectl apply -f "$tmpfile"
  kubectl rollout status \
    deployment \
    otlp-sink \
    --namespace otlp-sink \
    --timeout 1m
}

install_operator_configuration_resource() {
  if [[ "${USE_OTLP_SINK:-}" = "true" ]]; then
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.otlpsink.yaml
  elif [[ "${USE_TOKEN:-}" = true ]]; then
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.token.yaml
  else
    kubectl apply -f test-resources/customresources/dash0operatorconfiguration/dash0operatorconfiguration.secret.yaml
  fi

  echo "waiting for the operator configuration resource to become available"
  kubectl wait dash0operatorconfigurations.operator.dash0.com/dash0-operator-configuration-resource --for condition=Available
}

install_monitoring_resource() {
  local additional_namespaces=${1:-false}
  kubectl apply -n ${target_namespace} -f test-resources/customresources/dash0monitoring/dash0monitoring.yaml
  if [[ "$additional_namespaces" = "true" ]]; then
    kubectl apply -n test-namespace-2 -f test-resources/customresources/dash0monitoring/dash0monitoring-2.yaml
    kubectl apply -n test-namespace-3 -f test-resources/customresources/dash0monitoring/dash0monitoring-3.yaml
    kubectl apply -n routed-namespace -f test-resources/customresources/dash0monitoring/dash0monitoring.yaml
  fi

  echo "waiting for the monitoring resource to become available"
  kubectl wait --namespace ${target_namespace} dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
  if [[ "$additional_namespaces" = "true" ]]; then
    kubectl wait --namespace test-namespace-2 dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
    kubectl wait --namespace test-namespace-3 dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
    kubectl wait --namespace routed-namespace dash0monitorings.operator.dash0.com/dash0-monitoring-resource --for condition=Available
  fi
}

deploy_application_under_monitoring() {
  if [[ "${DEPLOY_APPLICATION_UNDER_MONITORING:-}" = "false" ]]; then
    return
  fi

  runtime_under_test="${1:-nodejs}"
  echo "STEP $step_counter: deploy application under monitoring ($runtime_under_test)"
  if [[ "${runtime_under_test:-}" = "nodejs" ]]; then
    test-resources/node.js/express/deploy.sh "${target_namespace}" "${kind}"
    if [[ "$additional_namespaces" = true ]]; then
      test-resources/node.js/express/deploy.sh test-namespace-2 daemonset
      test-resources/node.js/express/deploy.sh test-namespace-3 statefulset
      test-resources/node.js/express/deploy.sh routed-namespace replicaset
    fi
  elif [[ "${runtime_under_test:-}" = "jvm" ]]; then
    build_test_application test-resources/jvm/spring-boot dash0-operator-jvm-spring-boot-test-app
    pushd test-resources/jvm/spring-boot > /dev/null
    kubectl apply -n "${target_namespace}" -f "${kind}".yaml
    if [[ "$additional_namespaces" = true ]]; then
      kubectl apply -n test-namespace-2 -f daemonset.yaml
      kubectl apply -n test-namespace-3 -f statefulset.yaml
      kubectl apply -n routed-namespace -f replicaset.yaml
    fi
    popd > /dev/null
  elif [[ "${runtime_under_test:-}" = "python" ]]; then
    build_test_application test-resources/python/flask dash0-operator-python-flask-test-app
    pushd test-resources/python/flask > /dev/null
    kubectl apply -n "${target_namespace}" -f "${kind}".yaml
    if [[ "$additional_namespaces" = true ]]; then
      kubectl apply -n test-namespace-2 -f daemonset.yaml
      kubectl apply -n test-namespace-3 -f statefulset.yaml
      kubectl apply -n routed-namespace -f replicaset.yaml
    fi
    popd > /dev/null
  else
    echo "Error: unknown runtime: $runtime_under_test"
    exit 1
  fi
  finish_step
}

build_test_application() {
  if [[ "${SKIP_DOCKER_BUILD:-}" = "true" ]]; then
    return
  fi

  local app_path="$1"
  local image_name="$2"

  pushd "$app_path" > /dev/null
  echo "building container image for application under monitoring ($image_name)"
  docker build . -t "$image_name"
  popd > /dev/null

  if [[ "$is_kind_cluster" = "true" ]]; then
    echo loading test image into Kind cluster
    kind load docker-image \
      --name "$kind_cluster_name" \
      "$image_name:latest"
  fi
}

install_third_party_crds() {
  kubectl apply --server-side -f https://raw.githubusercontent.com/perses/perses-operator/refs/tags/v0.2.0/config/crd/bases/perses.dev_persesdashboards.yaml
  kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.85.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
}

deploy_dash0_api_sync_resources() {
  if [[ "${DEPLOY_SYNTHETIC_CHECK:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Dash0 synthetic check resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/dash0syntheticcheck/dash0syntheticcheck.yaml
    finish_step
  fi
  if [[ "${DEPLOY_VIEW:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Dash0 view resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/dash0view/dash0view.yaml
    finish_step
  fi
  if [[ "${DEPLOY_PERSES_DASHBOARD:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Perses dashboard resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/persesdashboard/persesdashboard.yaml
    finish_step
  fi
  if [[ "${DEPLOY_PROMETHEUS_RULE:-}" = "true" ]]; then
    echo "STEP $step_counter: deploy a Prometheus rule resource to namespace ${target_namespace}"
    kubectl apply -n "$target_namespace" -f test-resources/customresources/prometheusrule/prometheusrule.yaml
    finish_step
  fi
}

trim() {
  local var="$*"
  # remove leading whitespace characters
  var="${var#"${var%%[![:space:]]*}"}"
  # remove trailing whitespace characters
  var="${var%"${var##*[![:space:]]}"}"
  printf '%s' "$var"
}

finish_scenario() {
  if [[ "$is_minikube_cluster" = "true" ]]; then
    echo 'Detected minikube cluster, will run `eval $(minikube docker-env -u)`'
    eval $(minikube docker-env -u)
  fi
}

finish_step() {
  ((step_counter++))
  echo
  echo
}
